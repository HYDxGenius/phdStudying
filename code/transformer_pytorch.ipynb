{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_47273708/article/details/134228655?spm=1001.2101.3001.6650.4&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-4-134228655-blog-124489562.235%5Ev43%5Epc_blog_bottom_relevance_base6&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-4-134228655-blog-124489562.235%5Ev43%5Epc_blog_bottom_relevance_base6&utm_relevant_index=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enc_inputs: \n",
      " tensor([[1, 2, 3, 4, 0],\n",
      "        [1, 2, 3, 5, 0]])\n",
      " dec_inputs: \n",
      " tensor([[6, 1, 2, 3, 4, 8],\n",
      "        [6, 1, 2, 3, 5, 8]])\n",
      " dec_outputs: \n",
      " tensor([[1, 2, 3, 4, 8, 7],\n",
      "        [1, 2, 3, 5, 8, 7]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "# S: 起始标记\n",
    "# E: 结束标记\n",
    "# P：意为padding，将当前序列补齐至最长序列长度的占位符\n",
    "sentence = [\n",
    "    # enc_input   dec_input    dec_output\n",
    "    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n",
    "    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n",
    "]\n",
    " \n",
    "# 词典，padding用0来表示\n",
    "# 源词典\n",
    "src_vocab = {'P':0, 'ich':1,'mochte':2,'ein':3,'bier':4,'cola':5}\n",
    "src_vocab_size = len(src_vocab) # 6\n",
    "# 目标词典（包含特殊符）\n",
    "tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n",
    "# 反向映射词典，idx ——> word (序号和对应的词)\n",
    "idx2word = {v:k for k,v in tgt_vocab.items()}\n",
    "tgt_vocab_size = len(tgt_vocab) # 9\n",
    " \n",
    "src_len = 5 # 输入序列enc_input的最长序列长度，其实就是最长的那句话的token数\n",
    "tgt_len = 6 # 输出序列dec_input/dec_output的最长序列长度\n",
    "\n",
    "# 构建模型输入的Tensor\n",
    "def make_data(sentence):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [],[],[]\n",
    "    for i in range(len(sentence)):\n",
    "        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n",
    "        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n",
    "        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n",
    "        \n",
    "        enc_inputs.append(enc_input)\n",
    "        dec_inputs.append(dec_input)\n",
    "        dec_outputs.append(dec_output)\n",
    "        \n",
    "    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n",
    "    return torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)\n",
    " \n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n",
    " \n",
    "print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n",
    "print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n",
    "print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 使用Dataset加载数据\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self,enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet,self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 我们前面的enc_inputs.shape = [2,5],所以这个返回的是2\n",
    "        print(enc_inputs.shape[0]) #矩阵shape就是 0：行 1：列\n",
    "        return self.enc_inputs.shape[0] \n",
    "    \n",
    "    # 根据idx返回的是一组 enc_input, dec_input, dec_output\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    " \n",
    "# 构建DataLoader\n",
    "loader = Data.DataLoader(dataset=MyDataSet(enc_inputs,dec_inputs, dec_outputs),batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数\n",
    "Transformer包含Encoder和Decoder\n",
    "\n",
    "Encoder和Decoder各自包含6个Layer\n",
    "\n",
    "Encoder Layer中包含 Self Attention 和 FFN 两个Sub Layer\n",
    "\n",
    "Decoder Layer中包含 Masked Self Attention、 Cross Attention、 FFN 三个Sub Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来表示一个词的向量长度\n",
    "d_model = 512\n",
    " \n",
    "# FFN的隐藏层神经元个数\n",
    "d_ff = 2048\n",
    " \n",
    "# 分头后的q、k、v词向量长度，依照原文我们都设为64\n",
    "# 原文：queries and kes of dimention d_k,and values of dimension d_v .所以q和k的长度都用d_k来表示\n",
    "d_k = d_v = 64\n",
    " \n",
    "# Encoder Layer 和 Decoder Layer的个数\n",
    "n_layers = 6\n",
    " \n",
    "# 多头注意力中head的个数，原文：we employ h = 8 parallel attention layers, or heads\n",
    "n_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding\n",
    "用于为输入的词向量进行位置编码\n",
    "The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000): \n",
    "        # dropout是原文的0.1，max_len原文没找到\n",
    "        #max_len是假设的一个句子最多包含5000个token\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 开始位置编码部分,先生成一个max_len * d_model 的矩阵，即5000 * 512\n",
    "        # 5000是一个句子中最多的token数，512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # pos：[max_len,1],即[5000,1]\n",
    "        # 先把括号内的分式求出来,pos是[5000,1],分母是[256],通过广播机制相乘后是[5000,256]\n",
    "        div_term = pos / pow(10000.0,torch.arange(0, d_model, 2).float() / d_model)\n",
    "        \n",
    "        # 再取正余弦\n",
    "        pe[:, 0::2] = torch.sin(div_term)\n",
    "        pe[:, 1::2] = torch.cos(div_term)\n",
    "        # 一个句子要做一次pe，一个batch中会有多个句子，所以增加一维用来和输入的一个batch的数据相加时做广播\n",
    "        pe = pe.unsqueeze(0) # [5000,512] -> [1,5000,512] \n",
    "        # 将pe作为固定参数保存到缓冲区，不会被更新\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''x: [batch_size, seq_len, d_model]'''\n",
    "        # 5000是我们预定义的最大的seq_len，就是说我们把最多的情况pe都算好了，用的时候用多少就取多少\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x) # return: [batch_size, seq_len, d_model], 和输入的形状相同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask  防止关注padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为enc_input和dec_input做一个mask，把占位符P的token（就是0） mask掉\n",
    "# 返回一个[batch_size, len_q, len_k]大小的布尔张量，True是需要mask掉的位置\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size() #batchsize 和lenq 等于seqq的长宽\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # seq_k.data.eq(0)返回一个等大的布尔张量，seq_k元素等于0的位置为True,否则为False\n",
    "    # 然后扩维以保证后续操作的兼容(广播)\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # pad_attn_mask: [batch_size,1,len_k]\n",
    "    # 要为每一个q提供一份k，所以把第二维度扩展了q次\n",
    "    # 另注意expand并非真正加倍了内存，只是重复了引用，对任意引用的修改都会修改原始值\n",
    "    # 这里是因为我们不会修改这个mask所以用它来节省内存\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k) # return: [batch_size, len_q, len_k]\n",
    "    # 返回的是batch_size个 len_q * len_k的矩阵，内容是True和False，\n",
    "    # 第i行第j列表示的是query的第i个词对key的第j个词的注意力是否无意义，若无意义则为True，有意义的为False（即被padding的位置是True）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于获取对后续位置的掩码，防止在预测过程中看到未来时刻的输入\n",
    "# 原文：to prevent positions from attending to subsequent positions\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"seq: [batch_size, tgt_len]\"\"\"\n",
    "    # batch_size个 tgt_len * tgt_len的mask矩阵\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # np.triu 是生成一个 upper triangular matrix 上三角矩阵，k是相对于主对角线的偏移量\n",
    "    # k=1意为不包含主对角线（从主对角线向上偏移1开始）\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte() # 因为只有0、1所以用byte节省内存\n",
    "    return subsequence_mask  # return: [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled dot-product attention此函数用于计算缩放点积注意力，在MultiHeadAttention中被调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductionAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductionAttention,self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        ''' \n",
    "         Q [batch size, n_heads, len_q, d_k]\n",
    "         K [batch size, n_heads, len_k, d_k]\n",
    "         V [batch size, n_heads, len_v=len_k, d_k]\n",
    "         全文两处用到注意力，一处是self attention，\n",
    "         另一处是co attention，\n",
    "         前者不必说，后者的k和v都是encoder的输出，所以k和v的形状总是相同的\n",
    "         attn_mask: [batch size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "         #1) 计算attention score = QK^T/根号dk\n",
    "         #K.shape = [batch_size, n_heads, len_k, d_k]\n",
    "         #K^T:\n",
    "         #K.shape → [batch_size, n_heads, d_k, len_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        \n",
    "        # scores: [batch_size, n_heads, len_q, len_k] transpose(-1,-2)是交换后面两个维度\n",
    "\n",
    "        #2) 进行mask 和 softmax\n",
    "        scores.masked_fill_(attn_mask, -1e9)# mask为True的位置会被设为-1e9\n",
    "        attn = nn. Softmax(dim=-1)(scores) # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        # 3) 乘V得到最终的加权\n",
    "        context = torch.matmul(attn, V) # context: [batch_size, n_heads, len_q, d_v]\n",
    "        '''\n",
    "        得出的context是每个维度(d_1-d_v)都考虑了在当前维度(这一列)当前token对所有token的注意力后更新的新的值，\n",
    "        换言之每个维度d是相互独立的，每个维度考虑自己的所有token的注意力，所以可以理解成1列扩展到多列\n",
    "        返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，\n",
    "        只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，\n",
    "        每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来\n",
    "        '''\n",
    "        return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(MultiHeadAttention,self)._init_()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        ''' \n",
    "        input_Q:[batch_size, len_q, d_model]\n",
    "        len_q是作为query的句子的长度，比如enc_inputs（2,5,512）作为输入，那句子长度5就是len_q\n",
    "        input_K:[batch_size, len_k, d_model]\n",
    "        input_V:[batch_size, len_v(len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # 1）linear projection \n",
    "        # 用 W_Q 线性变换 input_Q，把 d_model 维度映射到 n_heads * d_k 维度。\n",
    "        # 用 .view() 重新 reshape 形状，把 n_heads 维度拆分出来，使其可以进行多头计算。\n",
    "        # input_Q [batch_size, seq_len, d_model] ->  [batch_size, n_heads, seq_len, d_k/d_v]\n",
    "\n",
    "        # .view(batch_size, -1, n_heads, d_k) 重新 reshape：\n",
    "        # -1 表示保持 seq_len 维度不变\n",
    "        # n_heads, d_k 把注意力头数 n_heads 分离出来\n",
    "        # 变换后形状：[batch_size, seq_len, n_heads, d_k]\n",
    "        #然后再进行转1，2 交换 nheads和seqlen\n",
    "\n",
    "        \n",
    "\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2) \n",
    "    \n",
    "        # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2) \n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        #2）计算attention\n",
    "        # 自我复制n_heads次，为每个头准备一份mask\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        context = ScaledDotProductionAttention()(Q, K, V, attn_mask)\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "\n",
    "        #3)concat\n",
    "        context = torch.cat([context[:,i,:,:] for i in range (context.size(1))], dim=-1)\n",
    "        output = self.concat(context)# [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual)\n",
    "    #output: [batch_size, len_q, d_model]\n",
    "    #context = context.transpose(1.2).reshape(batch_size, -1, d_model)\n",
    "    #output = self.linear(context)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForward Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        #MLP\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "\n",
    "        )\n",
    "    def forward(self,inputs):\n",
    "        #inputs:[b_s, seq_len, d_m]\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual )\n",
    "    # return： [batch_size, seq_len, d_model] 形状不变\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # 残差连接\n",
    "        x = self.fc1(x)  # 第一层线性变换\n",
    "        x = self.relu(x)  # 非线性激活\n",
    "        x = self.fc2(x)  # 第二层线性变换\n",
    "        return self.layer_norm(x + residual)  # 残差连接 + 归一化\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs:[bs, erc_len, d_model]\n",
    "        enc_self_attn_mask: [bs, src_len, src_len]\n",
    "        QKV 均为 enc_inputs\n",
    "\n",
    "        '''\n",
    "\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) \n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs # enc_outputs: [batch_size, src_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Stack封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        # 直接调的现成接口完成词向量的编码，输入是类别数和每一个类别要映射成的向量长度\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        #定义 输入词嵌入层（Word Embedding）。\n",
    "        # src_vocab_size：源语言（德语）词汇表大小。\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        #位置编码层，给输入 Token 添加位置信息。\n",
    "\n",
    "        self.layer = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])#重复运行layers数((六次))\n",
    "        #创建 n_layers=6 层 Encoder。\n",
    "        #nn.ModuleList([]) 用于存储多个 EncoderLayer() 层。\n",
    "\n",
    "        '''等价于\n",
    "            self.layers = nn.ModuleList([\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer()\n",
    "                ])'''\n",
    "    def forward(self, enc_inputs):\n",
    "        ''' enc_inputs: [batchsize, src_len]'''\n",
    "        enc_outputs = self.src_emb(enc_inputs)\n",
    "        # [batch_size, src_len] -> [batch_size, src_len, d_model]\n",
    "        # 把 enc_inputs（Token ID）转换成 d_model 维向量。\n",
    "        enc_outputs = self.pos_emb(enc_outputs)\n",
    "        #加上位置编码，确保模型能识别 Token 的顺序。\n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "\n",
    "        # Encoder中是self attention, 传入QK都是enc_inputs\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs,enc_inputs)\n",
    "        # enc_self_attn_mask: [batch_size, src_len, src_len]计算mask\n",
    "        for layer in self.layers:\n",
    "            enc_outputs = layers(enc_outputs,enc_self_attn_mask)\n",
    "        return enc_outputs\n",
    "    # enc_outputs: [batch_size, src_len, d_model]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "    def forward(self,dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len] 前者是Q后者是K\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "        dec_outputs = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "\n",
    "        return dec_outputs\n",
    "    # dec_outputs: [batch_size, tgt_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size,d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range (n_layers)])\n",
    "\n",
    "    def forward (self, dec_inputs, enc_inputs,enc_outputs):\n",
    "\n",
    "        '''\n",
    "        这三个参数对应的不是Q、K、V，dec_inputs是Q，enc_outputs是K和V，enc_inputs是用来计算padding mask的\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_inpus: [batch_size, src_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "\n",
    "        '''\n",
    "        #forward() 计算 Decoder 输出\n",
    "        dec_outputs = self.tgt_emb(dec_inputs)\n",
    "        dec_outputs = self.pos_emb(dec_outputs)\n",
    "\n",
    "\n",
    "        #计算 Decoder Self-Attention 的 Mask\n",
    "        \n",
    "        #屏蔽pad位置\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs,dec_inputs).cuda()\n",
    "\n",
    "        #屏蔽未来信息\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda()\n",
    "\n",
    "        # 将两个mask叠加，布尔值可以视为0和1，和大于0的位置是需要被mask掉的，赋为True，\n",
    "        # 和为0的位置是有意义的为False\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),0).cuda()\n",
    "\n",
    "\n",
    "        #计算Enc-Decattention的Mask\n",
    "\n",
    "        #co-attention 部分 传入的是enc_inputs \n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "        #enc_inputs 计算pad mask \n",
    "        #enc_output 计算QK相关性\n",
    "\n",
    "        '''\n",
    "        get_attn_pad_mask(seq_q, seq_k) 计算的是 K 的 PAD Mask，用于屏蔽 PAD 位置的注意力分数。\n",
    "        在 Encoder-Decoder Attention 里：\n",
    "        Q（Query） 来自 Decoder（dec_inputs）。\n",
    "        K 和 V（Key & Value） 来自 Encoder（enc_outputs）。\n",
    "        enc_outputs 和 enc_inputs 形状相同，enc_outputs 是 enc_inputs 经过 Encoder 处理后的表示，\n",
    "        但 PAD 位置没变。\n",
    "        Mask 只关心哪些位置是 PAD，所以 enc_inputs 可以直接用来计算 PAD 位置，而不需要 enc_outputs。\n",
    "        '''\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "            dec_inputs = layer(dec_inputs,enc_inputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            \n",
    "        return dec_outputs  # [batch_size, tgt_len, d_model]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整合整个transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().cuda()\n",
    "        self.decoder = Decoder().cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size).cuda()\n",
    "\n",
    "    def forward(self,enc_inputs,dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        enc_outputs = self.encoder(enc_inputs)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs)\n",
    "\n",
    "        # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "\n",
    "        # 解散batch，一个batch中有batch_size个句子，每个句子有tgt_len个词（即tgt_len行），\n",
    "        # 现在让他们按行依次排布，如前tgt_len行是第一个句子的每个词的预测概率，\n",
    "        # 再往下tgt_len行是第二个句子的，一直到batch_size * tgt_len行\n",
    "\n",
    "        return dec_logits.view(-1,dec_logits.size(-1))#  [batch_size * tgt_len, tgt_vocab_size]\n",
    "        '''最后变形的原因是：nn.CrossEntropyLoss接收的输入的第二个维度必须是类别\n",
    "        dec_logits.size(-1) 代表 tgt_vocab_size，即 dec_logits 的最后一维。\n",
    "        -1 让 PyTorch 自动计算 前两维 batch_size * tgt_len 的大小。\n",
    "        最终形状变为 [batch_size * tgt_len, tgt_vocab_size]。\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Transformer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, tgt_vocab_size)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\76922\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\76922\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\76922\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\76922\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\76922\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = Transformer().cuda()\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(),lr = 1e-3, momentum=0.99)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len] [2,5]\n",
    "        dec_inputs: [batch_size, tgt_len] [2,6]\n",
    "        dec_outputs: [batch_size, tgt_len] [2,6]\n",
    "        '''\n",
    "        enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda()\n",
    "        outputs = model (enc_inputs, dec_inputs)\n",
    "        #outputs: [batch_size * tgt_len, tgt_vocab_size], dec_outputs: [batch_size, tgt_len]\n",
    "        loss = criterion(outputs,dec_outputs.view(-1))# 将dec_outputs展平成一维张量\n",
    "        #更新权重\n",
    "        optimizer.zero_gard()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch[{epoch+1}/1000], Loss:{loss.item()}')\n",
    "torch.save(model,'MyPt.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.5.1+cpu\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 检查是否支持 CUDA\n",
    "print(torch.__version__)          # 检查 PyTorch 版本\n",
    "print(torch.version.cuda)         # 检查 CUDA 版本\n",
    "print(torch.cuda.device_count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
