{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_47273708/article/details/134228655?spm=1001.2101.3001.6650.4&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-4-134228655-blog-124489562.235%5Ev43%5Epc_blog_bottom_relevance_base6&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-4-134228655-blog-124489562.235%5Ev43%5Epc_blog_bottom_relevance_base6&utm_relevant_index=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enc_inputs: \n",
      " tensor([[1, 2, 3, 4, 0],\n",
      "        [1, 2, 3, 5, 0]])\n",
      " dec_inputs: \n",
      " tensor([[6, 1, 2, 3, 4, 8],\n",
      "        [6, 1, 2, 3, 5, 8]])\n",
      " dec_outputs: \n",
      " tensor([[1, 2, 3, 4, 8, 7],\n",
      "        [1, 2, 3, 5, 8, 7]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "# S: 起始标记\n",
    "# E: 结束标记\n",
    "# P：意为padding，将当前序列补齐至最长序列长度的占位符\n",
    "sentence = [\n",
    "    # enc_input   dec_input    dec_output\n",
    "    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n",
    "    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n",
    "]\n",
    " \n",
    "# 词典，padding用0来表示\n",
    "# 源词典\n",
    "src_vocab = {'P':0, 'ich':1,'mochte':2,'ein':3,'bier':4,'cola':5}\n",
    "src_vocab_size = len(src_vocab) # 6\n",
    "# 目标词典（包含特殊符）\n",
    "tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n",
    "# 反向映射词典，idx ——> word (序号和对应的词)\n",
    "idx2word = {v:k for k,v in tgt_vocab.items()}\n",
    "tgt_vocab_size = len(tgt_vocab) # 9\n",
    " \n",
    "src_len = 5 # 输入序列enc_input的最长序列长度，其实就是最长的那句话的token数\n",
    "tgt_len = 6 # 输出序列dec_input/dec_output的最长序列长度\n",
    "\n",
    "# 构建模型输入的Tensor\n",
    "def make_data(sentence):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [],[],[]\n",
    "    for i in range(len(sentence)):\n",
    "        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n",
    "        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n",
    "        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n",
    "        \n",
    "        enc_inputs.append(enc_input)\n",
    "        dec_inputs.append(dec_input)\n",
    "        dec_outputs.append(dec_output)\n",
    "        \n",
    "    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n",
    "    return torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)\n",
    " \n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n",
    " \n",
    "print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n",
    "print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n",
    "print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Dataset加载数据\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self,enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet,self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 我们前面的enc_inputs.shape = [2,5],所以这个返回的是2\n",
    "        # print(enc_inputs.shape[0]) #矩阵shape就是 0：行 1：列\n",
    "        return self.enc_inputs.shape[0] \n",
    "    \n",
    "    # 根据idx返回的是一组 enc_input, dec_input, dec_output\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    " \n",
    "# 构建DataLoader\n",
    "loader = Data.DataLoader(dataset=MyDataSet(enc_inputs,dec_inputs, dec_outputs),batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数\n",
    "Transformer包含Encoder和Decoder\n",
    "\n",
    "Encoder和Decoder各自包含6个Layer\n",
    "\n",
    "Encoder Layer中包含 Self Attention 和 FFN 两个Sub Layer\n",
    "\n",
    "Decoder Layer中包含 Masked Self Attention、 Cross Attention、 FFN 三个Sub Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来表示一个词的向量长度\n",
    "d_model = 512\n",
    " \n",
    "# FFN的隐藏层神经元个数\n",
    "d_ff = 2048\n",
    " \n",
    "# 分头后的q、k、v词向量长度，依照原文我们都设为64\n",
    "# 原文：queries and kes of dimention d_k,and values of dimension d_v .所以q和k的长度都用d_k来表示\n",
    "d_k = d_v = 64\n",
    " \n",
    "# Encoder Layer 和 Decoder Layer的个数\n",
    "n_layers = 6\n",
    " \n",
    "# 多头注意力中head的个数，原文：we employ h = 8 parallel attention layers, or heads\n",
    "n_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding\n",
    "用于为输入的词向量进行位置编码\n",
    "The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000): \n",
    "        # dropout是原文的0.1，max_len原文没找到\n",
    "        #max_len是假设的一个句子最多包含5000个token\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 开始位置编码部分,先生成一个max_len * d_model 的矩阵，即5000 * 512\n",
    "        # 5000是一个句子中最多的token数，512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # pos：[max_len,1],即[5000,1]\n",
    "        # 先把括号内的分式求出来,pos是[5000,1],分母是[256],通过广播机制相乘后是[5000,256]\n",
    "        div_term = pos / pow(10000.0,torch.arange(0, d_model, 2).float() / d_model)\n",
    "        \n",
    "        # 再取正余弦\n",
    "        pe[:, 0::2] = torch.sin(div_term)\n",
    "        pe[:, 1::2] = torch.cos(div_term)\n",
    "        # 一个句子要做一次pe，一个batch中会有多个句子，所以增加一维用来和输入的一个batch的数据相加时做广播\n",
    "        pe = pe.unsqueeze(0) # [5000,512] -> [1,5000,512] \n",
    "        # 将pe作为固定参数保存到缓冲区，不会被更新\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''x: [batch_size, seq_len, d_model]'''\n",
    "        # 5000是我们预定义的最大的seq_len，就是说我们把最多的情况pe都算好了，用的时候用多少就取多少\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x) # return: [batch_size, seq_len, d_model], 和输入的形状相同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask  防止关注padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为enc_input和dec_input做一个mask，把占位符P的token（就是0） mask掉\n",
    "# 返回一个[batch_size, len_q, len_k]大小的布尔张量，True是需要mask掉的位置\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size() #batchsize 和lenq 等于seqq的长宽\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # seq_k.data.eq(0)返回一个等大的布尔张量，seq_k元素等于0的位置为True,否则为False\n",
    "    # 然后扩维以保证后续操作的兼容(广播)\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # pad_attn_mask: [batch_size,1,len_k]\n",
    "    # 要为每一个q提供一份k，所以把第二维度扩展了q次\n",
    "    # 另注意expand并非真正加倍了内存，只是重复了引用，对任意引用的修改都会修改原始值\n",
    "    # 这里是因为我们不会修改这个mask所以用它来节省内存\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k) # return: [batch_size, len_q, len_k]\n",
    "    # 返回的是batch_size个 len_q * len_k的矩阵，内容是True和False，\n",
    "    # 第i行第j列表示的是query的第i个词对key的第j个词的注意力是否无意义，若无意义则为True，有意义的为False（即被padding的位置是True）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于获取对后续位置的掩码，防止在预测过程中看到未来时刻的输入\n",
    "# 原文：to prevent positions from attending to subsequent positions\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"seq: [batch_size, tgt_len]\"\"\"\n",
    "    # batch_size个 tgt_len * tgt_len的mask矩阵\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # np.triu 是生成一个 upper triangular matrix 上三角矩阵，k是相对于主对角线的偏移量\n",
    "    # k=1意为不包含主对角线（从主对角线向上偏移1开始）\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte() # 因为只有0、1所以用byte节省内存\n",
    "    return subsequence_mask  # return: [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled dot-product attention此函数用于计算缩放点积注意力，在MultiHeadAttention中被调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductionAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductionAttention,self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        ''' \n",
    "         Q [batch size, n_heads, len_q, d_k]\n",
    "         K [batch size, n_heads, len_k, d_k]\n",
    "         V [batch size, n_heads, len_v=len_k, d_k]\n",
    "         全文两处用到注意力，一处是self attention，\n",
    "         另一处是co attention，\n",
    "         前者不必说，后者的k和v都是encoder的输出，所以k和v的形状总是相同的\n",
    "         attn_mask: [batch size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "         #1) 计算attention score = QK^T/根号dk\n",
    "         #K.shape = [batch_size, n_heads, len_k, d_k]\n",
    "         #K^T:\n",
    "         #K.shape → [batch_size, n_heads, d_k, len_k]\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        \n",
    "        # scores: [batch_size, n_heads, len_q, len_k] transpose(-1,-2)是交换后面两个维度\n",
    "\n",
    "        #2) 进行mask 和 softmax\n",
    "        scores.masked_fill_(attn_mask, -1e9)# mask为True的位置会被设为-1e9\n",
    "        attn = nn. Softmax(dim=-1)(scores) # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        # 3) 乘V得到最终的加权\n",
    "        context = torch.matmul(attn, V) # context: [batch_size, n_heads, len_q, d_v]\n",
    "        '''\n",
    "        得出的context是每个维度(d_1-d_v)都考虑了在当前维度(这一列)当前token对所有token的注意力后更新的新的值，\n",
    "        换言之每个维度d是相互独立的，每个维度考虑自己的所有token的注意力，所以可以理解成1列扩展到多列\n",
    "        返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，\n",
    "        只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，\n",
    "        每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来\n",
    "        '''\n",
    "        return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        ''' \n",
    "        input_Q:[batch_size, len_q, d_model]\n",
    "        len_q是作为query的句子的长度，比如enc_inputs（2,5,512）作为输入，那句子长度5就是len_q\n",
    "        input_K:[batch_size, len_k, d_model]\n",
    "        input_V:[batch_size, len_v(len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # 1）linear projection \n",
    "        # 用 W_Q 线性变换 input_Q，把 d_model 维度映射到 n_heads * d_k 维度。\n",
    "        # 用 .view() 重新 reshape 形状，把 n_heads 维度拆分出来，使其可以进行多头计算。\n",
    "        # input_Q [batch_size, seq_len, d_model] ->  [batch_size, n_heads, seq_len, d_k/d_v]\n",
    "\n",
    "        # .view(batch_size, -1, n_heads, d_k) 重新 reshape：\n",
    "        # -1 表示保持 seq_len 维度不变\n",
    "        # n_heads, d_k 把注意力头数 n_heads 分离出来\n",
    "        # 变换后形状：[batch_size, seq_len, n_heads, d_k]\n",
    "        #然后再进行转1，2 交换 nheads和seqlen\n",
    "        assert len(input_Q.shape) == 3, f\"Expected shape [batch_size, seq_len, d_model], but got {input_Q.shape}\"\n",
    "\n",
    "        input_Q = input_Q.float()  # ✅ 确保转换为 float\n",
    "        input_K = input_K.float()\n",
    "        input_V = input_V.float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2) \n",
    "    \n",
    "        # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2) \n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        #2）计算attention\n",
    "        # 自我复制n_heads次，为每个头准备一份mask\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        context = ScaledDotProductionAttention()(Q, K, V, attn_mask)\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "\n",
    "        #3)concat\n",
    "        context = torch.cat([context[:,i,:,:] for i in range (context.size(1))], dim=-1)\n",
    "        output = self.concat(context)# [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual)\n",
    "    #output: [batch_size, len_q, d_model]\n",
    "    #context = context.transpose(1.2).reshape(batch_size, -1, d_model)\n",
    "    #output = self.linear(context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForward Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        #MLP\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "\n",
    "        )\n",
    "    def forward(self,inputs):\n",
    "        #inputs:[b_s, seq_len, d_m]\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual )\n",
    "    # return： [batch_size, seq_len, d_model] 形状不变\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # 残差连接\n",
    "        x = self.fc1(x)  # 第一层线性变换\n",
    "        x = self.relu(x)  # 非线性激活\n",
    "        x = self.fc2(x)  # 第二层线性变换\n",
    "        return self.layer_norm(x + residual)  # 残差连接 + 归一化\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs:[bs, erc_len, d_model]\n",
    "        enc_self_attn_mask: [bs, src_len, src_len]\n",
    "        QKV 均为 enc_inputs\n",
    "\n",
    "        '''\n",
    "\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) \n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs # enc_outputs: [batch_size, src_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Stack封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        # 直接调的现成接口完成词向量的编码，输入是类别数和每一个类别要映射成的向量长度\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        #定义 输入词嵌入层（Word Embedding）。\n",
    "        # src_vocab_size：源语言（德语）词汇表大小。\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        #位置编码层，给输入 Token 添加位置信息。\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])#重复运行layers数((六次))\n",
    "        #创建 n_layers=6 层 Encoder。\n",
    "        #nn.ModuleList([]) 用于存储多个 EncoderLayer() 层。\n",
    "\n",
    "        '''等价于\n",
    "            self.layers = nn.ModuleList([\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer(),\n",
    "                EncoderLayer()\n",
    "                ])'''\n",
    "    def forward(self, enc_inputs):\n",
    "        ''' enc_inputs: [batchsize, src_len]'''\n",
    "        enc_outputs = self.src_emb(enc_inputs).float()\n",
    "        # [batch_size, src_len] -> [batch_size, src_len, d_model]\n",
    "        # 把 enc_inputs（Token ID）转换成 d_model 维向量。\n",
    "        enc_outputs = self.pos_emb(enc_outputs)\n",
    "        #加上位置编码，确保模型能识别 Token 的顺序。\n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "\n",
    "        # Encoder中是self attention, 传入QK都是enc_inputs\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs,enc_inputs)\n",
    "        # enc_self_attn_mask: [batch_size, src_len, src_len]计算mask\n",
    "        for layer in self.layers:\n",
    "            enc_outputs = layer(enc_outputs,enc_self_attn_mask)\n",
    "        return enc_outputs\n",
    "    # enc_outputs: [batch_size, src_len, d_model]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PositionwiseFeedForward()\n",
    "    def forward(self,dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len] 前者是Q后者是K\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "        dec_outputs = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "\n",
    "        return dec_outputs\n",
    "    # dec_outputs: [batch_size, tgt_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size,d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range (n_layers)])\n",
    "\n",
    "    def forward (self, dec_inputs, enc_inputs,enc_outputs):\n",
    "\n",
    "        '''\n",
    "        这三个参数对应的不是Q、K、V，dec_inputs是Q，enc_outputs是K和V，enc_inputs是用来计算padding mask的\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_inpus: [batch_size, src_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "\n",
    "        '''\n",
    "        #forward() 计算 Decoder 输出\n",
    "        dec_outputs = self.tgt_emb(dec_inputs).float()\n",
    "        dec_outputs = self.pos_emb(dec_outputs)\n",
    "\n",
    "\n",
    "        #计算 Decoder Self-Attention 的 Mask\n",
    "        \n",
    "        #屏蔽pad位置\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs,dec_inputs).cuda()\n",
    "\n",
    "        #屏蔽未来信息\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda()\n",
    "\n",
    "        # 将两个mask叠加，布尔值可以视为0和1，和大于0的位置是需要被mask掉的，赋为True，\n",
    "        # 和为0的位置是有意义的为False\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),0).cuda()\n",
    "\n",
    "\n",
    "        #计算Enc-Decattention的Mask\n",
    "\n",
    "        #co-attention 部分 传入的是enc_inputs \n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "        #enc_inputs 计算pad mask \n",
    "        #enc_output 计算QK相关性\n",
    "\n",
    "        '''\n",
    "        get_attn_pad_mask(seq_q, seq_k) 计算的是 K 的 PAD Mask，用于屏蔽 PAD 位置的注意力分数。\n",
    "        在 Encoder-Decoder Attention 里：\n",
    "        Q（Query） 来自 Decoder（dec_inputs）。\n",
    "        K 和 V（Key & Value） 来自 Encoder（enc_outputs）。\n",
    "        enc_outputs 和 enc_inputs 形状相同，enc_outputs 是 enc_inputs 经过 Encoder 处理后的表示，\n",
    "        但 PAD 位置没变。\n",
    "        Mask 只关心哪些位置是 PAD，所以 enc_inputs 可以直接用来计算 PAD 位置，而不需要 enc_outputs。\n",
    "        '''\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "            dec_outputs = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            \n",
    "        return dec_outputs  # [batch_size, tgt_len, d_model]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整合整个transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().cuda()\n",
    "        self.decoder = Decoder().cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size).cuda()\n",
    "\n",
    "    def forward(self,enc_inputs,dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        enc_outputs = self.encoder(enc_inputs)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs)\n",
    "\n",
    "        # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "\n",
    "        # 解散batch，一个batch中有batch_size个句子，每个句子有tgt_len个词（即tgt_len行），\n",
    "        # 现在让他们按行依次排布，如前tgt_len行是第一个句子的每个词的预测概率，\n",
    "        # 再往下tgt_len行是第二个句子的，一直到batch_size * tgt_len行\n",
    "\n",
    "        return dec_logits.view(-1,dec_logits.size(-1))#  [batch_size * tgt_len, tgt_vocab_size]\n",
    "        '''最后变形的原因是：nn.CrossEntropyLoss接收的输入的第二个维度必须是类别\n",
    "        dec_logits.size(-1) 代表 tgt_vocab_size，即 dec_logits 的最后一维。\n",
    "        -1 让 PyTorch 自动计算 前两维 batch_size * tgt_len 的大小。\n",
    "        最终形状变为 [batch_size * tgt_len, tgt_vocab_size]。\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1000], Loss:2.5237057209014893\n",
      "Epoch[2/1000], Loss:2.289264678955078\n",
      "Epoch[3/1000], Loss:2.1217026710510254\n",
      "Epoch[4/1000], Loss:1.9151992797851562\n",
      "Epoch[5/1000], Loss:1.7438710927963257\n",
      "Epoch[6/1000], Loss:1.541401982307434\n",
      "Epoch[7/1000], Loss:1.2106322050094604\n",
      "Epoch[8/1000], Loss:1.0677357912063599\n",
      "Epoch[9/1000], Loss:0.7822077870368958\n",
      "Epoch[10/1000], Loss:0.6379414200782776\n",
      "Epoch[11/1000], Loss:0.4821396768093109\n",
      "Epoch[12/1000], Loss:0.38205015659332275\n",
      "Epoch[13/1000], Loss:0.27682721614837646\n",
      "Epoch[14/1000], Loss:0.21960507333278656\n",
      "Epoch[15/1000], Loss:0.1650388091802597\n",
      "Epoch[16/1000], Loss:0.14117150008678436\n",
      "Epoch[17/1000], Loss:0.14162719249725342\n",
      "Epoch[18/1000], Loss:0.1187485083937645\n",
      "Epoch[19/1000], Loss:0.1022670567035675\n",
      "Epoch[20/1000], Loss:0.08820021152496338\n",
      "Epoch[21/1000], Loss:0.08239931613206863\n",
      "Epoch[22/1000], Loss:0.0615466944873333\n",
      "Epoch[23/1000], Loss:0.049465734511613846\n",
      "Epoch[24/1000], Loss:0.04720669612288475\n",
      "Epoch[25/1000], Loss:0.04748940095305443\n",
      "Epoch[26/1000], Loss:0.03976776823401451\n",
      "Epoch[27/1000], Loss:0.03347228839993477\n",
      "Epoch[28/1000], Loss:0.03449975326657295\n",
      "Epoch[29/1000], Loss:0.03471764922142029\n",
      "Epoch[30/1000], Loss:0.026258280500769615\n",
      "Epoch[31/1000], Loss:0.02557487227022648\n",
      "Epoch[32/1000], Loss:0.017573803663253784\n",
      "Epoch[33/1000], Loss:0.017111459746956825\n",
      "Epoch[34/1000], Loss:0.014464152045547962\n",
      "Epoch[35/1000], Loss:0.016896389424800873\n",
      "Epoch[36/1000], Loss:0.011384106241166592\n",
      "Epoch[37/1000], Loss:0.014343214221298695\n",
      "Epoch[38/1000], Loss:0.01969570480287075\n",
      "Epoch[39/1000], Loss:0.01159654464572668\n",
      "Epoch[40/1000], Loss:0.011984463781118393\n",
      "Epoch[41/1000], Loss:0.009760009124875069\n",
      "Epoch[42/1000], Loss:0.007567130494862795\n",
      "Epoch[43/1000], Loss:0.006951033603399992\n",
      "Epoch[44/1000], Loss:0.007681740447878838\n",
      "Epoch[45/1000], Loss:0.0049581448547542095\n",
      "Epoch[46/1000], Loss:0.005104842595756054\n",
      "Epoch[47/1000], Loss:0.006552161183208227\n",
      "Epoch[48/1000], Loss:0.004144832491874695\n",
      "Epoch[49/1000], Loss:0.003702966496348381\n",
      "Epoch[50/1000], Loss:0.0033604612108319998\n",
      "Epoch[51/1000], Loss:0.0020588154438883066\n",
      "Epoch[52/1000], Loss:0.0025115811731666327\n",
      "Epoch[53/1000], Loss:0.002680472331121564\n",
      "Epoch[54/1000], Loss:0.0024168374948203564\n",
      "Epoch[55/1000], Loss:0.0017637470737099648\n",
      "Epoch[56/1000], Loss:0.0017002872191369534\n",
      "Epoch[57/1000], Loss:0.0026121139526367188\n",
      "Epoch[58/1000], Loss:0.0014679316664114594\n",
      "Epoch[59/1000], Loss:0.0010118790669366717\n",
      "Epoch[60/1000], Loss:0.0018226333195343614\n",
      "Epoch[61/1000], Loss:0.001035198918543756\n",
      "Epoch[62/1000], Loss:0.0009828063193708658\n",
      "Epoch[63/1000], Loss:0.0015431806677952409\n",
      "Epoch[64/1000], Loss:0.0011744186049327254\n",
      "Epoch[65/1000], Loss:0.0010420973412692547\n",
      "Epoch[66/1000], Loss:0.0009695095359347761\n",
      "Epoch[67/1000], Loss:0.0008415346965193748\n",
      "Epoch[68/1000], Loss:0.0007328856736421585\n",
      "Epoch[69/1000], Loss:0.0005759310442954302\n",
      "Epoch[70/1000], Loss:0.0008085728040896356\n",
      "Epoch[71/1000], Loss:0.0010302899172529578\n",
      "Epoch[72/1000], Loss:0.0005954134394414723\n",
      "Epoch[73/1000], Loss:0.0006123841158114374\n",
      "Epoch[74/1000], Loss:0.0006457728450186551\n",
      "Epoch[75/1000], Loss:0.00063989037880674\n",
      "Epoch[76/1000], Loss:0.000631903822068125\n",
      "Epoch[77/1000], Loss:0.0006074869888834655\n",
      "Epoch[78/1000], Loss:0.0005314579466357827\n",
      "Epoch[79/1000], Loss:0.00042790392762981355\n",
      "Epoch[80/1000], Loss:0.0005431509925983846\n",
      "Epoch[81/1000], Loss:0.0006791316554881632\n",
      "Epoch[82/1000], Loss:0.0006942152394913137\n",
      "Epoch[83/1000], Loss:0.0005686068325303495\n",
      "Epoch[84/1000], Loss:0.0007509609567932785\n",
      "Epoch[85/1000], Loss:0.0006563954520970583\n",
      "Epoch[86/1000], Loss:0.0006051296950317919\n",
      "Epoch[87/1000], Loss:0.0004217577225062996\n",
      "Epoch[88/1000], Loss:0.00049118377501145\n",
      "Epoch[89/1000], Loss:0.0004050572169944644\n",
      "Epoch[90/1000], Loss:0.0006541362963616848\n",
      "Epoch[91/1000], Loss:0.0005064259748905897\n",
      "Epoch[92/1000], Loss:0.00046132257557474077\n",
      "Epoch[93/1000], Loss:0.0005488101742230356\n",
      "Epoch[94/1000], Loss:0.0005669165402650833\n",
      "Epoch[95/1000], Loss:0.0003517542500048876\n",
      "Epoch[96/1000], Loss:0.00037244902341626585\n",
      "Epoch[97/1000], Loss:0.0003801319980993867\n",
      "Epoch[98/1000], Loss:0.0005298450123518705\n",
      "Epoch[99/1000], Loss:0.0004930756404064596\n",
      "Epoch[100/1000], Loss:0.0003705100098159164\n",
      "Epoch[101/1000], Loss:0.0004565078706946224\n",
      "Epoch[102/1000], Loss:0.00036611862014979124\n",
      "Epoch[103/1000], Loss:0.00031060169567354023\n",
      "Epoch[104/1000], Loss:0.0004211205814499408\n",
      "Epoch[105/1000], Loss:0.00036819264641962945\n",
      "Epoch[106/1000], Loss:0.0002968110784422606\n",
      "Epoch[107/1000], Loss:0.0003000379365403205\n",
      "Epoch[108/1000], Loss:0.0002888189337681979\n",
      "Epoch[109/1000], Loss:0.0002982703736051917\n",
      "Epoch[110/1000], Loss:0.000393768772482872\n",
      "Epoch[111/1000], Loss:0.00037508923560380936\n",
      "Epoch[112/1000], Loss:0.0003679755318444222\n",
      "Epoch[113/1000], Loss:0.00037982556386850774\n",
      "Epoch[114/1000], Loss:0.00027573146508075297\n",
      "Epoch[115/1000], Loss:0.00028631874010898173\n",
      "Epoch[116/1000], Loss:0.0002155718393623829\n",
      "Epoch[117/1000], Loss:0.00038555238279514015\n",
      "Epoch[118/1000], Loss:0.0002550364297349006\n",
      "Epoch[119/1000], Loss:0.0002737565082497895\n",
      "Epoch[120/1000], Loss:0.00020106066949665546\n",
      "Epoch[121/1000], Loss:0.000345967331668362\n",
      "Epoch[122/1000], Loss:0.00017651972302701324\n",
      "Epoch[123/1000], Loss:0.00017350450798403472\n",
      "Epoch[124/1000], Loss:0.0002314305311301723\n",
      "Epoch[125/1000], Loss:0.0002438670489937067\n",
      "Epoch[126/1000], Loss:0.00023506686557084322\n",
      "Epoch[127/1000], Loss:0.0002729453844949603\n",
      "Epoch[128/1000], Loss:0.00019362066814210266\n",
      "Epoch[129/1000], Loss:0.00022886214719619602\n",
      "Epoch[130/1000], Loss:0.00022426596842706203\n",
      "Epoch[131/1000], Loss:0.0001561256794957444\n",
      "Epoch[132/1000], Loss:0.0003224054235033691\n",
      "Epoch[133/1000], Loss:0.00020063789270352572\n",
      "Epoch[134/1000], Loss:0.00013893069990444928\n",
      "Epoch[135/1000], Loss:0.00019985635299235582\n",
      "Epoch[136/1000], Loss:0.0001548776781419292\n",
      "Epoch[137/1000], Loss:0.00018834973161574453\n",
      "Epoch[138/1000], Loss:0.0001733824028633535\n",
      "Epoch[139/1000], Loss:8.903456182451919e-05\n",
      "Epoch[140/1000], Loss:0.00011597040429478511\n",
      "Epoch[141/1000], Loss:0.00012561293260660022\n",
      "Epoch[142/1000], Loss:0.0001098062566597946\n",
      "Epoch[143/1000], Loss:0.00015068672655615956\n",
      "Epoch[144/1000], Loss:0.00013871886767446995\n",
      "Epoch[145/1000], Loss:6.090177339501679e-05\n",
      "Epoch[146/1000], Loss:9.528623922960833e-05\n",
      "Epoch[147/1000], Loss:5.83288783673197e-05\n",
      "Epoch[148/1000], Loss:0.00014058219676371664\n",
      "Epoch[149/1000], Loss:0.00018631183775141835\n",
      "Epoch[150/1000], Loss:0.00011606889893300831\n",
      "Epoch[151/1000], Loss:9.510456584393978e-05\n",
      "Epoch[152/1000], Loss:6.337468948913738e-05\n",
      "Epoch[153/1000], Loss:8.25411407276988e-05\n",
      "Epoch[154/1000], Loss:0.00013972731539979577\n",
      "Epoch[155/1000], Loss:8.209209772758186e-05\n",
      "Epoch[156/1000], Loss:0.0001190628026961349\n",
      "Epoch[157/1000], Loss:9.266159031540155e-05\n",
      "Epoch[158/1000], Loss:7.543490210082382e-05\n",
      "Epoch[159/1000], Loss:4.389645255287178e-05\n",
      "Epoch[160/1000], Loss:6.4802763517946e-05\n",
      "Epoch[161/1000], Loss:5.3223662689561024e-05\n",
      "Epoch[162/1000], Loss:4.828677629120648e-05\n",
      "Epoch[163/1000], Loss:5.028350278735161e-05\n",
      "Epoch[164/1000], Loss:8.04535739007406e-05\n",
      "Epoch[165/1000], Loss:5.851700916537084e-05\n",
      "Epoch[166/1000], Loss:4.843560964218341e-05\n",
      "Epoch[167/1000], Loss:5.886517828912474e-05\n",
      "Epoch[168/1000], Loss:6.0840535297757015e-05\n",
      "Epoch[169/1000], Loss:8.922759298002347e-05\n",
      "Epoch[170/1000], Loss:5.526885433937423e-05\n",
      "Epoch[171/1000], Loss:6.37721095699817e-05\n",
      "Epoch[172/1000], Loss:3.77673604816664e-05\n",
      "Epoch[173/1000], Loss:4.19198622694239e-05\n",
      "Epoch[174/1000], Loss:4.883245856035501e-05\n",
      "Epoch[175/1000], Loss:4.4860174966743216e-05\n",
      "Epoch[176/1000], Loss:5.973853330942802e-05\n",
      "Epoch[177/1000], Loss:5.123565279063769e-05\n",
      "Epoch[178/1000], Loss:4.641905252356082e-05\n",
      "Epoch[179/1000], Loss:7.379981252597645e-05\n",
      "Epoch[180/1000], Loss:4.399637327878736e-05\n",
      "Epoch[181/1000], Loss:4.182070915703662e-05\n",
      "Epoch[182/1000], Loss:4.7859499318292364e-05\n",
      "Epoch[183/1000], Loss:4.0688348235562444e-05\n",
      "Epoch[184/1000], Loss:6.0771973949158564e-05\n",
      "Epoch[185/1000], Loss:4.496924520935863e-05\n",
      "Epoch[186/1000], Loss:5.281676203594543e-05\n",
      "Epoch[187/1000], Loss:2.9106109650456347e-05\n",
      "Epoch[188/1000], Loss:4.027138129458763e-05\n",
      "Epoch[189/1000], Loss:3.60694139089901e-05\n",
      "Epoch[190/1000], Loss:3.368551188032143e-05\n",
      "Epoch[191/1000], Loss:4.920079663861543e-05\n",
      "Epoch[192/1000], Loss:4.30227373726666e-05\n",
      "Epoch[193/1000], Loss:6.225270044524223e-05\n",
      "Epoch[194/1000], Loss:5.91620737395715e-05\n",
      "Epoch[195/1000], Loss:4.2943429434672e-05\n",
      "Epoch[196/1000], Loss:2.7248510377830826e-05\n",
      "Epoch[197/1000], Loss:5.067026359029114e-05\n",
      "Epoch[198/1000], Loss:4.81281203974504e-05\n",
      "Epoch[199/1000], Loss:3.285098136984743e-05\n",
      "Epoch[200/1000], Loss:6.27482877462171e-05\n",
      "Epoch[201/1000], Loss:4.27840459451545e-05\n",
      "Epoch[202/1000], Loss:2.595723344711587e-05\n",
      "Epoch[203/1000], Loss:2.1854591977898963e-05\n",
      "Epoch[204/1000], Loss:5.016427530790679e-05\n",
      "Epoch[205/1000], Loss:3.165896487189457e-05\n",
      "Epoch[206/1000], Loss:4.4363794586388394e-05\n",
      "Epoch[207/1000], Loss:3.850297798635438e-05\n",
      "Epoch[208/1000], Loss:5.2051316743018106e-05\n",
      "Epoch[209/1000], Loss:2.3771657652105205e-05\n",
      "Epoch[210/1000], Loss:3.4479711757740006e-05\n",
      "Epoch[211/1000], Loss:3.4738175600068644e-05\n",
      "Epoch[212/1000], Loss:0.00010132192255696282\n",
      "Epoch[213/1000], Loss:2.952318209281657e-05\n",
      "Epoch[214/1000], Loss:4.554498809739016e-05\n",
      "Epoch[215/1000], Loss:1.9828159565804526e-05\n",
      "Epoch[216/1000], Loss:2.600668085506186e-05\n",
      "Epoch[217/1000], Loss:3.7678164517274126e-05\n",
      "Epoch[218/1000], Loss:6.899418804096058e-05\n",
      "Epoch[219/1000], Loss:5.125734969624318e-05\n",
      "Epoch[220/1000], Loss:0.00015066257037688047\n",
      "Epoch[221/1000], Loss:5.6611363106640056e-05\n",
      "Epoch[222/1000], Loss:4.592332697939128e-05\n",
      "Epoch[223/1000], Loss:3.6158697184873745e-05\n",
      "Epoch[224/1000], Loss:3.1341078283730894e-05\n",
      "Epoch[225/1000], Loss:5.2657054766314104e-05\n",
      "Epoch[226/1000], Loss:3.858216223306954e-05\n",
      "Epoch[227/1000], Loss:2.5341336368001066e-05\n",
      "Epoch[228/1000], Loss:0.0001425536029273644\n",
      "Epoch[229/1000], Loss:3.552283669705503e-05\n",
      "Epoch[230/1000], Loss:5.16329200763721e-05\n",
      "Epoch[231/1000], Loss:4.005147275165655e-05\n",
      "Epoch[232/1000], Loss:4.956729753757827e-05\n",
      "Epoch[233/1000], Loss:5.8944002375938e-05\n",
      "Epoch[234/1000], Loss:4.520750371739268e-05\n",
      "Epoch[235/1000], Loss:6.486110942205414e-05\n",
      "Epoch[236/1000], Loss:2.8827918868046254e-05\n",
      "Epoch[237/1000], Loss:4.012127828900702e-05\n",
      "Epoch[238/1000], Loss:2.7159070668858476e-05\n",
      "Epoch[239/1000], Loss:0.00010942963854176924\n",
      "Epoch[240/1000], Loss:5.4177711717784405e-05\n",
      "Epoch[241/1000], Loss:3.7022506148787215e-05\n",
      "Epoch[242/1000], Loss:3.5075416235486045e-05\n",
      "Epoch[243/1000], Loss:5.756229438702576e-05\n",
      "Epoch[244/1000], Loss:3.7698293454013765e-05\n",
      "Epoch[245/1000], Loss:3.120218025287613e-05\n",
      "Epoch[246/1000], Loss:3.536424628691748e-05\n",
      "Epoch[247/1000], Loss:4.4075524783693254e-05\n",
      "Epoch[248/1000], Loss:3.243373430450447e-05\n",
      "Epoch[249/1000], Loss:0.00013471436977852136\n",
      "Epoch[250/1000], Loss:5.615223562926985e-05\n",
      "Epoch[251/1000], Loss:4.4491298467619345e-05\n",
      "Epoch[252/1000], Loss:5.294500806485303e-05\n",
      "Epoch[253/1000], Loss:2.3493592379963957e-05\n",
      "Epoch[254/1000], Loss:3.581071723601781e-05\n",
      "Epoch[255/1000], Loss:5.724584843846969e-05\n",
      "Epoch[256/1000], Loss:0.00014908880984876305\n",
      "Epoch[257/1000], Loss:6.837493128841743e-05\n",
      "Epoch[258/1000], Loss:3.544357241480611e-05\n",
      "Epoch[259/1000], Loss:5.875624265172519e-05\n",
      "Epoch[260/1000], Loss:2.9582834031316452e-05\n",
      "Epoch[261/1000], Loss:4.490868377615698e-05\n",
      "Epoch[262/1000], Loss:4.096521661267616e-05\n",
      "Epoch[263/1000], Loss:6.878309795865789e-05\n",
      "Epoch[264/1000], Loss:5.453438279801048e-05\n",
      "Epoch[265/1000], Loss:3.6148503568256274e-05\n",
      "Epoch[266/1000], Loss:5.80609921598807e-05\n",
      "Epoch[267/1000], Loss:4.798774534719996e-05\n",
      "Epoch[268/1000], Loss:3.1201991077978164e-05\n",
      "Epoch[269/1000], Loss:3.230402944609523e-05\n",
      "Epoch[270/1000], Loss:5.032240369473584e-05\n",
      "Epoch[271/1000], Loss:2.8310998459346592e-05\n",
      "Epoch[272/1000], Loss:4.4661032006843016e-05\n",
      "Epoch[273/1000], Loss:3.1470182875636965e-05\n",
      "Epoch[274/1000], Loss:5.455477730720304e-05\n",
      "Epoch[275/1000], Loss:5.246847285889089e-05\n",
      "Epoch[276/1000], Loss:3.3903372241184115e-05\n",
      "Epoch[277/1000], Loss:3.405237293918617e-05\n",
      "Epoch[278/1000], Loss:3.5324406781001016e-05\n",
      "Epoch[279/1000], Loss:3.0109193176031113e-05\n",
      "Epoch[280/1000], Loss:3.136089435429312e-05\n",
      "Epoch[281/1000], Loss:4.20393880631309e-05\n",
      "Epoch[282/1000], Loss:2.8698646929115057e-05\n",
      "Epoch[283/1000], Loss:2.386107917118352e-05\n",
      "Epoch[284/1000], Loss:5.004291233490221e-05\n",
      "Epoch[285/1000], Loss:4.0082235500449315e-05\n",
      "Epoch[286/1000], Loss:2.703989412111696e-05\n",
      "Epoch[287/1000], Loss:3.0327981221489608e-05\n",
      "Epoch[288/1000], Loss:3.1231698812916875e-05\n",
      "Epoch[289/1000], Loss:3.0357659852597862e-05\n",
      "Epoch[290/1000], Loss:1.8884265955421142e-05\n",
      "Epoch[291/1000], Loss:3.242391176172532e-05\n",
      "Epoch[292/1000], Loss:2.8390903025865555e-05\n",
      "Epoch[293/1000], Loss:2.535112616897095e-05\n",
      "Epoch[294/1000], Loss:3.0149145459290594e-05\n",
      "Epoch[295/1000], Loss:3.091328835580498e-05\n",
      "Epoch[296/1000], Loss:3.084447962464765e-05\n",
      "Epoch[297/1000], Loss:2.9145718144718558e-05\n",
      "Epoch[298/1000], Loss:2.4844603103701957e-05\n",
      "Epoch[299/1000], Loss:2.364262218179647e-05\n",
      "Epoch[300/1000], Loss:2.0026733182021417e-05\n",
      "Epoch[301/1000], Loss:3.102320624748245e-05\n",
      "Epoch[302/1000], Loss:4.932984302286059e-05\n",
      "Epoch[303/1000], Loss:4.138356962357648e-05\n",
      "Epoch[304/1000], Loss:2.8251784897292964e-05\n",
      "Epoch[305/1000], Loss:7.4538285844028e-05\n",
      "Epoch[306/1000], Loss:3.291044777142815e-05\n",
      "Epoch[307/1000], Loss:3.8979978853603825e-05\n",
      "Epoch[308/1000], Loss:6.110010872362182e-05\n",
      "Epoch[309/1000], Loss:3.505598579067737e-05\n",
      "Epoch[310/1000], Loss:6.59761208225973e-05\n",
      "Epoch[311/1000], Loss:8.785623504081741e-05\n",
      "Epoch[312/1000], Loss:6.105117063270882e-05\n",
      "Epoch[313/1000], Loss:2.6563191568129696e-05\n",
      "Epoch[314/1000], Loss:4.4492673623608425e-05\n",
      "Epoch[315/1000], Loss:4.441286364453845e-05\n",
      "Epoch[316/1000], Loss:4.994507253286429e-05\n",
      "Epoch[317/1000], Loss:2.1079444195493124e-05\n",
      "Epoch[318/1000], Loss:2.3443877580575645e-05\n",
      "Epoch[319/1000], Loss:3.467825081315823e-05\n",
      "Epoch[320/1000], Loss:4.8315938329324126e-05\n",
      "Epoch[321/1000], Loss:2.4238668629550375e-05\n",
      "Epoch[322/1000], Loss:4.681503924075514e-05\n",
      "Epoch[323/1000], Loss:4.7144509153440595e-05\n",
      "Epoch[324/1000], Loss:6.569694960489869e-05\n",
      "Epoch[325/1000], Loss:5.117720866110176e-05\n",
      "Epoch[326/1000], Loss:4.5118664274923503e-05\n",
      "Epoch[327/1000], Loss:2.850006967491936e-05\n",
      "Epoch[328/1000], Loss:1.4225425729819108e-05\n",
      "Epoch[329/1000], Loss:3.71718306269031e-05\n",
      "Epoch[330/1000], Loss:3.183776789228432e-05\n",
      "Epoch[331/1000], Loss:4.5376262278296053e-05\n",
      "Epoch[332/1000], Loss:4.1065766708925366e-05\n",
      "Epoch[333/1000], Loss:3.620827556005679e-05\n",
      "Epoch[334/1000], Loss:4.940943108522333e-05\n",
      "Epoch[335/1000], Loss:6.927240610821173e-05\n",
      "Epoch[336/1000], Loss:2.1049861970823258e-05\n",
      "Epoch[337/1000], Loss:4.3071999243693426e-05\n",
      "Epoch[338/1000], Loss:3.521485996316187e-05\n",
      "Epoch[339/1000], Loss:2.4973700419650413e-05\n",
      "Epoch[340/1000], Loss:2.613590913824737e-05\n",
      "Epoch[341/1000], Loss:3.9019392715999857e-05\n",
      "Epoch[342/1000], Loss:2.652335933817085e-05\n",
      "Epoch[343/1000], Loss:2.9751558031421155e-05\n",
      "Epoch[344/1000], Loss:1.85763983608922e-05\n",
      "Epoch[345/1000], Loss:3.110245961579494e-05\n",
      "Epoch[346/1000], Loss:3.699290391523391e-05\n",
      "Epoch[347/1000], Loss:3.567124076653272e-05\n",
      "Epoch[348/1000], Loss:3.105262294411659e-05\n",
      "Epoch[349/1000], Loss:1.893401531560812e-05\n",
      "Epoch[350/1000], Loss:3.78274307877291e-05\n",
      "Epoch[351/1000], Loss:5.7122855650959536e-05\n",
      "Epoch[352/1000], Loss:3.1281470000976697e-05\n",
      "Epoch[353/1000], Loss:2.152668639610056e-05\n",
      "Epoch[354/1000], Loss:3.574160291464068e-05\n",
      "Epoch[355/1000], Loss:2.861927896447014e-05\n",
      "Epoch[356/1000], Loss:2.161610063922126e-05\n",
      "Epoch[357/1000], Loss:2.5122695660684258e-05\n",
      "Epoch[358/1000], Loss:2.1447262042784132e-05\n",
      "Epoch[359/1000], Loss:3.84135746571701e-05\n",
      "Epoch[360/1000], Loss:3.526459840941243e-05\n",
      "Epoch[361/1000], Loss:1.8357868611929007e-05\n",
      "Epoch[362/1000], Loss:3.114221544819884e-05\n",
      "Epoch[363/1000], Loss:1.6440711988252588e-05\n",
      "Epoch[364/1000], Loss:3.136092345812358e-05\n",
      "Epoch[365/1000], Loss:2.490415317879524e-05\n",
      "Epoch[366/1000], Loss:4.4522006646730006e-05\n",
      "Epoch[367/1000], Loss:2.6503561457502656e-05\n",
      "Epoch[368/1000], Loss:2.8142509108874947e-05\n",
      "Epoch[369/1000], Loss:1.967909156519454e-05\n",
      "Epoch[370/1000], Loss:2.089093322865665e-05\n",
      "Epoch[371/1000], Loss:3.3417334634577855e-05\n",
      "Epoch[372/1000], Loss:1.644067742745392e-05\n",
      "Epoch[373/1000], Loss:1.988760595850181e-05\n",
      "Epoch[374/1000], Loss:1.9599638108047657e-05\n",
      "Epoch[375/1000], Loss:3.355621811351739e-05\n",
      "Epoch[376/1000], Loss:2.2907406673766673e-05\n",
      "Epoch[377/1000], Loss:3.076485882047564e-05\n",
      "Epoch[378/1000], Loss:1.712606717774179e-05\n",
      "Epoch[379/1000], Loss:2.624505759740714e-05\n",
      "Epoch[380/1000], Loss:1.61029147420777e-05\n",
      "Epoch[381/1000], Loss:1.907305522763636e-05\n",
      "Epoch[382/1000], Loss:1.6510190107510425e-05\n",
      "Epoch[383/1000], Loss:1.9351284208823927e-05\n",
      "Epoch[384/1000], Loss:2.8569758796948008e-05\n",
      "Epoch[385/1000], Loss:3.6059387639397755e-05\n",
      "Epoch[386/1000], Loss:3.035764348169323e-05\n",
      "Epoch[387/1000], Loss:2.3930406314320862e-05\n",
      "Epoch[388/1000], Loss:2.0225363186909817e-05\n",
      "Epoch[389/1000], Loss:2.2102874936535954e-05\n",
      "Epoch[390/1000], Loss:1.7910848328028806e-05\n",
      "Epoch[391/1000], Loss:2.2331294530886225e-05\n",
      "Epoch[392/1000], Loss:1.463271019019885e-05\n",
      "Epoch[393/1000], Loss:1.6798290744191036e-05\n",
      "Epoch[394/1000], Loss:2.390085137449205e-05\n",
      "Epoch[395/1000], Loss:1.851683009590488e-05\n",
      "Epoch[396/1000], Loss:4.118430661037564e-05\n",
      "Epoch[397/1000], Loss:1.6073154256446287e-05\n",
      "Epoch[398/1000], Loss:2.118886186508462e-05\n",
      "Epoch[399/1000], Loss:3.1649036827730015e-05\n",
      "Epoch[400/1000], Loss:3.1301475246436894e-05\n",
      "Epoch[401/1000], Loss:1.4324730727821589e-05\n",
      "Epoch[402/1000], Loss:8.563129995309282e-06\n",
      "Epoch[403/1000], Loss:1.728506504150573e-05\n",
      "Epoch[404/1000], Loss:1.899359267554246e-05\n",
      "Epoch[405/1000], Loss:1.4712175470776856e-05\n",
      "Epoch[406/1000], Loss:1.9698925825650804e-05\n",
      "Epoch[407/1000], Loss:2.5728535547386855e-05\n",
      "Epoch[408/1000], Loss:1.1990305210929364e-05\n",
      "Epoch[409/1000], Loss:3.3357202482875437e-05\n",
      "Epoch[410/1000], Loss:1.3758514796791133e-05\n",
      "Epoch[411/1000], Loss:1.824851096898783e-05\n",
      "Epoch[412/1000], Loss:1.4821441254753154e-05\n",
      "Epoch[413/1000], Loss:1.4642697351519018e-05\n",
      "Epoch[414/1000], Loss:1.4364518392540049e-05\n",
      "Epoch[415/1000], Loss:1.5506893760175444e-05\n",
      "Epoch[416/1000], Loss:2.3136017262004316e-05\n",
      "Epoch[417/1000], Loss:2.2261885533225723e-05\n",
      "Epoch[418/1000], Loss:2.0751946067321114e-05\n",
      "Epoch[419/1000], Loss:3.5076165659120306e-05\n",
      "Epoch[420/1000], Loss:2.5659188395366073e-05\n",
      "Epoch[421/1000], Loss:1.306315516558243e-05\n",
      "Epoch[422/1000], Loss:2.385116385994479e-05\n",
      "Epoch[423/1000], Loss:1.2139335922256578e-05\n",
      "Epoch[424/1000], Loss:2.638411388034001e-05\n",
      "Epoch[425/1000], Loss:1.2437331861292478e-05\n",
      "Epoch[426/1000], Loss:1.0361178283346817e-05\n",
      "Epoch[427/1000], Loss:1.3122767995810136e-05\n",
      "Epoch[428/1000], Loss:1.708638774289284e-05\n",
      "Epoch[429/1000], Loss:1.677841464697849e-05\n",
      "Epoch[430/1000], Loss:3.1152227165875956e-05\n",
      "Epoch[431/1000], Loss:1.7831394870881923e-05\n",
      "Epoch[432/1000], Loss:3.09338684019167e-05\n",
      "Epoch[433/1000], Loss:8.463778613077011e-06\n",
      "Epoch[434/1000], Loss:1.1801593245763797e-05\n",
      "Epoch[435/1000], Loss:1.7255244529224e-05\n",
      "Epoch[436/1000], Loss:1.206976503453916e-05\n",
      "Epoch[437/1000], Loss:1.0390963325335179e-05\n",
      "Epoch[438/1000], Loss:1.3281735846248921e-05\n",
      "Epoch[439/1000], Loss:2.4884400772862136e-05\n",
      "Epoch[440/1000], Loss:8.374388926313259e-06\n",
      "Epoch[441/1000], Loss:1.6341335140168667e-05\n",
      "Epoch[442/1000], Loss:1.2089650226698723e-05\n",
      "Epoch[443/1000], Loss:1.0698909136408474e-05\n",
      "Epoch[444/1000], Loss:1.800029531295877e-05\n",
      "Epoch[445/1000], Loss:1.7622800442040898e-05\n",
      "Epoch[446/1000], Loss:1.2953889381606132e-05\n",
      "Epoch[447/1000], Loss:1.2884390343970153e-05\n",
      "Epoch[448/1000], Loss:1.97088247659849e-05\n",
      "Epoch[449/1000], Loss:9.755203791428357e-06\n",
      "Epoch[450/1000], Loss:1.3242017303127795e-05\n",
      "Epoch[451/1000], Loss:9.973741725843865e-06\n",
      "Epoch[452/1000], Loss:1.7006903362926096e-05\n",
      "Epoch[453/1000], Loss:8.861145943228621e-06\n",
      "Epoch[454/1000], Loss:7.311438821488991e-06\n",
      "Epoch[455/1000], Loss:8.324704140250105e-06\n",
      "Epoch[456/1000], Loss:8.026699106267188e-06\n",
      "Epoch[457/1000], Loss:1.0917465260718018e-05\n",
      "Epoch[458/1000], Loss:8.751851055421866e-06\n",
      "Epoch[459/1000], Loss:1.08379936136771e-05\n",
      "Epoch[460/1000], Loss:1.3142631360096857e-05\n",
      "Epoch[461/1000], Loss:1.0053195182990748e-05\n",
      "Epoch[462/1000], Loss:8.980337952380069e-06\n",
      "Epoch[463/1000], Loss:1.0758519238152076e-05\n",
      "Epoch[464/1000], Loss:1.0768431820906699e-05\n",
      "Epoch[465/1000], Loss:1.6937219697865658e-05\n",
      "Epoch[466/1000], Loss:9.25848235056037e-06\n",
      "Epoch[467/1000], Loss:1.1523433386173565e-05\n",
      "Epoch[468/1000], Loss:1.4245299098547548e-05\n",
      "Epoch[469/1000], Loss:1.4900917449267581e-05\n",
      "Epoch[470/1000], Loss:1.554659684188664e-05\n",
      "Epoch[471/1000], Loss:1.4523472600558307e-05\n",
      "Epoch[472/1000], Loss:8.046557013585698e-06\n",
      "Epoch[473/1000], Loss:3.116215884801932e-05\n",
      "Epoch[474/1000], Loss:1.3649269931192975e-05\n",
      "Epoch[475/1000], Loss:8.08628374215914e-06\n",
      "Epoch[476/1000], Loss:1.0073080375150312e-05\n",
      "Epoch[477/1000], Loss:1.1255207937210798e-05\n",
      "Epoch[478/1000], Loss:8.930689546104986e-06\n",
      "Epoch[479/1000], Loss:1.2099592822778504e-05\n",
      "Epoch[480/1000], Loss:1.0390976967755705e-05\n",
      "Epoch[481/1000], Loss:8.344572961505037e-06\n",
      "Epoch[482/1000], Loss:2.001684879360255e-05\n",
      "Epoch[483/1000], Loss:1.4235284652386326e-05\n",
      "Epoch[484/1000], Loss:1.0579700756352395e-05\n",
      "Epoch[485/1000], Loss:1.1801585060311481e-05\n",
      "Epoch[486/1000], Loss:1.3271784155222122e-05\n",
      "Epoch[487/1000], Loss:1.1324746992613655e-05\n",
      "Epoch[488/1000], Loss:1.1046573490602896e-05\n",
      "Epoch[489/1000], Loss:1.3877754099667072e-05\n",
      "Epoch[490/1000], Loss:9.824726475926582e-06\n",
      "Epoch[491/1000], Loss:1.1066439583373722e-05\n",
      "Epoch[492/1000], Loss:9.089623745239805e-06\n",
      "Epoch[493/1000], Loss:1.0609499440761283e-05\n",
      "Epoch[494/1000], Loss:1.107640582631575e-05\n",
      "Epoch[495/1000], Loss:7.420707788696745e-06\n",
      "Epoch[496/1000], Loss:1.3688985745829996e-05\n",
      "Epoch[497/1000], Loss:1.2089664778613951e-05\n",
      "Epoch[498/1000], Loss:8.453845111944247e-06\n",
      "Epoch[499/1000], Loss:1.1662484212138224e-05\n",
      "Epoch[500/1000], Loss:9.218770173902158e-06\n",
      "Epoch[501/1000], Loss:6.854490038676886e-06\n",
      "Epoch[502/1000], Loss:1.022207197820535e-05\n",
      "Epoch[503/1000], Loss:1.0549872058618348e-05\n",
      "Epoch[504/1000], Loss:1.3639321878144983e-05\n",
      "Epoch[505/1000], Loss:8.781671567703597e-06\n",
      "Epoch[506/1000], Loss:1.1126096069347113e-05\n",
      "Epoch[507/1000], Loss:1.3500164641300216e-05\n",
      "Epoch[508/1000], Loss:1.0142627616005484e-05\n",
      "Epoch[509/1000], Loss:9.874386705632787e-06\n",
      "Epoch[510/1000], Loss:7.470391665265197e-06\n",
      "Epoch[511/1000], Loss:9.854541531240102e-06\n",
      "Epoch[512/1000], Loss:6.0895640672242735e-06\n",
      "Epoch[513/1000], Loss:9.55652376433136e-06\n",
      "Epoch[514/1000], Loss:1.207972763950238e-05\n",
      "Epoch[515/1000], Loss:1.5437311958521605e-05\n",
      "Epoch[516/1000], Loss:9.417450200999156e-06\n",
      "Epoch[517/1000], Loss:1.0778371688502375e-05\n",
      "Epoch[518/1000], Loss:1.3212160411057994e-05\n",
      "Epoch[519/1000], Loss:1.273534417123301e-05\n",
      "Epoch[520/1000], Loss:7.192245448095491e-06\n",
      "Epoch[521/1000], Loss:8.215433808800299e-06\n",
      "Epoch[522/1000], Loss:1.0192289664701093e-05\n",
      "Epoch[523/1000], Loss:7.450525572494371e-06\n",
      "Epoch[524/1000], Loss:7.152510079322383e-06\n",
      "Epoch[525/1000], Loss:1.1324690603942145e-05\n",
      "Epoch[526/1000], Loss:8.026677278394345e-06\n",
      "Epoch[527/1000], Loss:7.887615538493264e-06\n",
      "Epoch[528/1000], Loss:1.0510183528822381e-05\n",
      "Epoch[529/1000], Loss:5.195512130740099e-06\n",
      "Epoch[530/1000], Loss:1.4721844308951404e-05\n",
      "Epoch[531/1000], Loss:9.347917512059212e-06\n",
      "Epoch[532/1000], Loss:8.632647222839296e-06\n",
      "Epoch[533/1000], Loss:1.0102889973495621e-05\n",
      "Epoch[534/1000], Loss:1.0549830221862067e-05\n",
      "Epoch[535/1000], Loss:8.831349077809136e-06\n",
      "Epoch[536/1000], Loss:8.751877430768218e-06\n",
      "Epoch[537/1000], Loss:9.824646440392826e-06\n",
      "Epoch[538/1000], Loss:9.278376637666952e-06\n",
      "Epoch[539/1000], Loss:9.010159374156501e-06\n",
      "Epoch[540/1000], Loss:1.2198911463201512e-05\n",
      "Epoch[541/1000], Loss:1.2655747013923246e-05\n",
      "Epoch[542/1000], Loss:9.119429705606308e-06\n",
      "Epoch[543/1000], Loss:7.231977633637143e-06\n",
      "Epoch[544/1000], Loss:6.586262315977365e-06\n",
      "Epoch[545/1000], Loss:7.231980362121249e-06\n",
      "Epoch[546/1000], Loss:8.43397356220521e-06\n",
      "Epoch[547/1000], Loss:7.798217666277196e-06\n",
      "Epoch[548/1000], Loss:5.473660621646559e-06\n",
      "Epoch[549/1000], Loss:9.735323146742303e-06\n",
      "Epoch[550/1000], Loss:1.107641128328396e-05\n",
      "Epoch[551/1000], Loss:6.854488674434833e-06\n",
      "Epoch[552/1000], Loss:9.387641512148548e-06\n",
      "Epoch[553/1000], Loss:1.0500229109311476e-05\n",
      "Epoch[554/1000], Loss:7.202182587207062e-06\n",
      "Epoch[555/1000], Loss:9.328039595857263e-06\n",
      "Epoch[556/1000], Loss:6.665736236755038e-06\n",
      "Epoch[557/1000], Loss:1.4016793102200609e-05\n",
      "Epoch[558/1000], Loss:9.079644769371953e-06\n",
      "Epoch[559/1000], Loss:6.6955471993424e-06\n",
      "Epoch[560/1000], Loss:1.088764111045748e-05\n",
      "Epoch[561/1000], Loss:6.6657248680712655e-06\n",
      "Epoch[562/1000], Loss:8.980330676422454e-06\n",
      "Epoch[563/1000], Loss:6.50679248792585e-06\n",
      "Epoch[564/1000], Loss:8.642598004371393e-06\n",
      "Epoch[565/1000], Loss:8.79157869349001e-06\n",
      "Epoch[566/1000], Loss:1.1722088856913615e-05\n",
      "Epoch[567/1000], Loss:6.1988448578631505e-06\n",
      "Epoch[568/1000], Loss:8.692258234077599e-06\n",
      "Epoch[569/1000], Loss:8.46376769914059e-06\n",
      "Epoch[570/1000], Loss:1.1145925782329869e-05\n",
      "Epoch[571/1000], Loss:7.6790011007688e-06\n",
      "Epoch[572/1000], Loss:6.347852377075469e-06\n",
      "Epoch[573/1000], Loss:9.228707313013729e-06\n",
      "Epoch[574/1000], Loss:6.993564966251142e-06\n",
      "Epoch[575/1000], Loss:9.447205229662359e-06\n",
      "Epoch[576/1000], Loss:1.1841272680612747e-05\n",
      "Epoch[577/1000], Loss:1.0152484719583299e-05\n",
      "Epoch[578/1000], Loss:7.69887901697075e-06\n",
      "Epoch[579/1000], Loss:8.493590030411724e-06\n",
      "Epoch[580/1000], Loss:8.106168024824001e-06\n",
      "Epoch[581/1000], Loss:9.367770871904213e-06\n",
      "Epoch[582/1000], Loss:8.155803698173258e-06\n",
      "Epoch[583/1000], Loss:1.249686010851292e-05\n",
      "Epoch[584/1000], Loss:5.890889951842837e-06\n",
      "Epoch[585/1000], Loss:8.463782251055818e-06\n",
      "Epoch[586/1000], Loss:7.977015229698736e-06\n",
      "Epoch[587/1000], Loss:5.662407147610793e-06\n",
      "Epoch[588/1000], Loss:6.8445501710812096e-06\n",
      "Epoch[589/1000], Loss:8.175683433364611e-06\n",
      "Epoch[590/1000], Loss:1.0033347280113958e-05\n",
      "Epoch[591/1000], Loss:1.1662444194371346e-05\n",
      "Epoch[592/1000], Loss:9.59625504037831e-06\n",
      "Epoch[593/1000], Loss:9.914111615216825e-06\n",
      "Epoch[594/1000], Loss:1.0390937859483529e-05\n",
      "Epoch[595/1000], Loss:8.890964636520948e-06\n",
      "Epoch[596/1000], Loss:6.8445465331024025e-06\n",
      "Epoch[597/1000], Loss:5.324654466676293e-06\n",
      "Epoch[598/1000], Loss:7.679006557737011e-06\n",
      "Epoch[599/1000], Loss:5.582936410064576e-06\n",
      "Epoch[600/1000], Loss:5.7915403885999694e-06\n",
      "Epoch[601/1000], Loss:6.467062121373601e-06\n",
      "Epoch[602/1000], Loss:9.635984497435857e-06\n",
      "Epoch[603/1000], Loss:5.9107624110765755e-06\n",
      "Epoch[604/1000], Loss:9.556520126352552e-06\n",
      "Epoch[605/1000], Loss:9.337955816590693e-06\n",
      "Epoch[606/1000], Loss:5.970359779894352e-06\n",
      "Epoch[607/1000], Loss:5.235246590018505e-06\n",
      "Epoch[608/1000], Loss:9.457176020077895e-06\n",
      "Epoch[609/1000], Loss:7.380980605375953e-06\n",
      "Epoch[610/1000], Loss:8.076355697994586e-06\n",
      "Epoch[611/1000], Loss:4.788215392181883e-06\n",
      "Epoch[612/1000], Loss:6.556472271768143e-06\n",
      "Epoch[613/1000], Loss:8.106158020382281e-06\n",
      "Epoch[614/1000], Loss:9.069748557521962e-06\n",
      "Epoch[615/1000], Loss:5.2749824135389645e-06\n",
      "Epoch[616/1000], Loss:5.563063950830838e-06\n",
      "Epoch[617/1000], Loss:4.46039302914869e-06\n",
      "Epoch[618/1000], Loss:6.92402272761683e-06\n",
      "Epoch[619/1000], Loss:7.480302883777767e-06\n",
      "Epoch[620/1000], Loss:6.417394615709782e-06\n",
      "Epoch[621/1000], Loss:5.8908881328534335e-06\n",
      "Epoch[622/1000], Loss:7.251840088429162e-06\n",
      "Epoch[623/1000], Loss:7.450529210473178e-06\n",
      "Epoch[624/1000], Loss:9.973705346055795e-06\n",
      "Epoch[625/1000], Loss:8.970414455689024e-06\n",
      "Epoch[626/1000], Loss:4.837885171582457e-06\n",
      "Epoch[627/1000], Loss:9.506839887762908e-06\n",
      "Epoch[628/1000], Loss:6.705480700475164e-06\n",
      "Epoch[629/1000], Loss:8.31475426821271e-06\n",
      "Epoch[630/1000], Loss:8.443927072221413e-06\n",
      "Epoch[631/1000], Loss:6.427327662095195e-06\n",
      "Epoch[632/1000], Loss:6.3180555116559844e-06\n",
      "Epoch[633/1000], Loss:5.7617485254013445e-06\n",
      "Epoch[634/1000], Loss:5.940563369222218e-06\n",
      "Epoch[635/1000], Loss:7.748539246676955e-06\n",
      "Epoch[636/1000], Loss:5.990227236907231e-06\n",
      "Epoch[637/1000], Loss:4.788214937434532e-06\n",
      "Epoch[638/1000], Loss:7.3313008215336595e-06\n",
      "Epoch[639/1000], Loss:4.778279162565013e-06\n",
      "Epoch[640/1000], Loss:5.920686817262322e-06\n",
      "Epoch[641/1000], Loss:7.053158697090112e-06\n",
      "Epoch[642/1000], Loss:6.298188509390457e-06\n",
      "Epoch[643/1000], Loss:8.126035027089529e-06\n",
      "Epoch[644/1000], Loss:7.231963991216617e-06\n",
      "Epoch[645/1000], Loss:5.622674962069141e-06\n",
      "Epoch[646/1000], Loss:6.556466814799933e-06\n",
      "Epoch[647/1000], Loss:6.20877608525916e-06\n",
      "Epoch[648/1000], Loss:5.6922085605037864e-06\n",
      "Epoch[649/1000], Loss:5.274983777781017e-06\n",
      "Epoch[650/1000], Loss:7.39091046852991e-06\n",
      "Epoch[651/1000], Loss:8.40417851577513e-06\n",
      "Epoch[652/1000], Loss:7.311437002499588e-06\n",
      "Epoch[653/1000], Loss:1.1960496522078756e-05\n",
      "Epoch[654/1000], Loss:1.1434013686084654e-05\n",
      "Epoch[655/1000], Loss:4.6193358684831765e-06\n",
      "Epoch[656/1000], Loss:5.582936864811927e-06\n",
      "Epoch[657/1000], Loss:5.930628049100051e-06\n",
      "Epoch[658/1000], Loss:6.864415354357334e-06\n",
      "Epoch[659/1000], Loss:6.238584319362417e-06\n",
      "Epoch[660/1000], Loss:8.473707566736266e-06\n",
      "Epoch[661/1000], Loss:9.765124559635296e-06\n",
      "Epoch[662/1000], Loss:6.884280537633458e-06\n",
      "Epoch[663/1000], Loss:3.9239562283910345e-06\n",
      "Epoch[664/1000], Loss:5.483596851263428e-06\n",
      "Epoch[665/1000], Loss:6.914087407494662e-06\n",
      "Epoch[666/1000], Loss:9.83467270998517e-06\n",
      "Epoch[667/1000], Loss:6.4273203861375805e-06\n",
      "Epoch[668/1000], Loss:8.781681572145317e-06\n",
      "Epoch[669/1000], Loss:3.6656704196502687e-06\n",
      "Epoch[670/1000], Loss:6.496854894066928e-06\n",
      "Epoch[671/1000], Loss:7.837933480914216e-06\n",
      "Epoch[672/1000], Loss:8.642579814477358e-06\n",
      "Epoch[673/1000], Loss:5.7319448387715966e-06\n",
      "Epoch[674/1000], Loss:3.98356314690318e-06\n",
      "Epoch[675/1000], Loss:6.039897471055156e-06\n",
      "Epoch[676/1000], Loss:5.940560640738113e-06\n",
      "Epoch[677/1000], Loss:7.3909236562030856e-06\n",
      "Epoch[678/1000], Loss:1.2566494660859462e-05\n",
      "Epoch[679/1000], Loss:1.071879523806274e-05\n",
      "Epoch[680/1000], Loss:7.371055744442856e-06\n",
      "Epoch[681/1000], Loss:9.328028681920841e-06\n",
      "Epoch[682/1000], Loss:7.5697294050769415e-06\n",
      "Epoch[683/1000], Loss:6.318054602161283e-06\n",
      "Epoch[684/1000], Loss:8.294920007756446e-06\n",
      "Epoch[685/1000], Loss:4.202106993034249e-06\n",
      "Epoch[686/1000], Loss:8.722085112822242e-06\n",
      "Epoch[687/1000], Loss:8.990274181996938e-06\n",
      "Epoch[688/1000], Loss:5.841216534463456e-06\n",
      "Epoch[689/1000], Loss:7.4703893915284425e-06\n",
      "Epoch[690/1000], Loss:8.006821190065239e-06\n",
      "Epoch[691/1000], Loss:9.069739462574944e-06\n",
      "Epoch[692/1000], Loss:6.258450866880594e-06\n",
      "Epoch[693/1000], Loss:6.188909537740983e-06\n",
      "Epoch[694/1000], Loss:8.374387107323855e-06\n",
      "Epoch[695/1000], Loss:7.748539246676955e-06\n",
      "Epoch[696/1000], Loss:6.010093329678057e-06\n",
      "Epoch[697/1000], Loss:7.3313244683959056e-06\n",
      "Epoch[698/1000], Loss:7.410777016048087e-06\n",
      "Epoch[699/1000], Loss:6.278317869146122e-06\n",
      "Epoch[700/1000], Loss:9.079659321287181e-06\n",
      "Epoch[701/1000], Loss:7.271708909684094e-06\n",
      "Epoch[702/1000], Loss:8.493586392432917e-06\n",
      "Epoch[703/1000], Loss:5.871019311598502e-06\n",
      "Epoch[704/1000], Loss:5.592874003923498e-06\n",
      "Epoch[705/1000], Loss:5.821339073008858e-06\n",
      "Epoch[706/1000], Loss:4.748480478156125e-06\n",
      "Epoch[707/1000], Loss:4.519996309682028e-06\n",
      "Epoch[708/1000], Loss:5.731932560593123e-06\n",
      "Epoch[709/1000], Loss:5.572998361458303e-06\n",
      "Epoch[710/1000], Loss:8.970392627816182e-06\n",
      "Epoch[711/1000], Loss:7.351176918746205e-06\n",
      "Epoch[712/1000], Loss:7.192221801233245e-06\n",
      "Epoch[713/1000], Loss:7.231970357679529e-06\n",
      "Epoch[714/1000], Loss:6.198843038873747e-06\n",
      "Epoch[715/1000], Loss:4.490189439820824e-06\n",
      "Epoch[716/1000], Loss:8.702193554199766e-06\n",
      "Epoch[717/1000], Loss:6.208776540006511e-06\n",
      "Epoch[718/1000], Loss:5.980287369311554e-06\n",
      "Epoch[719/1000], Loss:3.6954734241589904e-06\n",
      "Epoch[720/1000], Loss:8.712132512300741e-06\n",
      "Epoch[721/1000], Loss:6.6856155171990395e-06\n",
      "Epoch[722/1000], Loss:6.327984010567889e-06\n",
      "Epoch[723/1000], Loss:4.897491635347251e-06\n",
      "Epoch[724/1000], Loss:6.1889118114777375e-06\n",
      "Epoch[725/1000], Loss:8.056485057750251e-06\n",
      "Epoch[726/1000], Loss:5.642539235850563e-06\n",
      "Epoch[727/1000], Loss:4.6888717406545766e-06\n",
      "Epoch[728/1000], Loss:4.212043222651118e-06\n",
      "Epoch[729/1000], Loss:8.702142622496467e-06\n",
      "Epoch[730/1000], Loss:5.573007456405321e-06\n",
      "Epoch[731/1000], Loss:6.5564636315684766e-06\n",
      "Epoch[732/1000], Loss:7.609468866576208e-06\n",
      "Epoch[733/1000], Loss:4.1623729885031935e-06\n",
      "Epoch[734/1000], Loss:6.715408744639717e-06\n",
      "Epoch[735/1000], Loss:6.586265953956172e-06\n",
      "Epoch[736/1000], Loss:6.546539680130081e-06\n",
      "Epoch[737/1000], Loss:4.212040494167013e-06\n",
      "Epoch[738/1000], Loss:4.897488452115795e-06\n",
      "Epoch[739/1000], Loss:5.761739885201678e-06\n",
      "Epoch[740/1000], Loss:4.281578640075168e-06\n",
      "Epoch[741/1000], Loss:7.152500984375365e-06\n",
      "Epoch[742/1000], Loss:4.917356363876024e-06\n",
      "Epoch[743/1000], Loss:7.0531473284063395e-06\n",
      "Epoch[744/1000], Loss:6.983624189160764e-06\n",
      "Epoch[745/1000], Loss:8.066409463935997e-06\n",
      "Epoch[746/1000], Loss:4.023297151434235e-06\n",
      "Epoch[747/1000], Loss:4.450458618521225e-06\n",
      "Epoch[748/1000], Loss:6.129306711954996e-06\n",
      "Epoch[749/1000], Loss:7.4604522524168715e-06\n",
      "Epoch[750/1000], Loss:5.354457243811339e-06\n",
      "Epoch[751/1000], Loss:7.102843028405914e-06\n",
      "Epoch[752/1000], Loss:6.715416475344682e-06\n",
      "Epoch[753/1000], Loss:6.7253481574880425e-06\n",
      "Epoch[754/1000], Loss:5.205445631872863e-06\n",
      "Epoch[755/1000], Loss:5.135904302733252e-06\n",
      "Epoch[756/1000], Loss:6.238573405425996e-06\n",
      "Epoch[757/1000], Loss:4.043164153699763e-06\n",
      "Epoch[758/1000], Loss:6.854474122519605e-06\n",
      "Epoch[759/1000], Loss:6.010097877151566e-06\n",
      "Epoch[760/1000], Loss:6.437260253733257e-06\n",
      "Epoch[761/1000], Loss:5.175645128474571e-06\n",
      "Epoch[762/1000], Loss:6.079632385080913e-06\n",
      "Epoch[763/1000], Loss:5.563069407799048e-06\n",
      "Epoch[764/1000], Loss:5.37432561031892e-06\n",
      "Epoch[765/1000], Loss:4.569670636556111e-06\n",
      "Epoch[766/1000], Loss:6.288253189268289e-06\n",
      "Epoch[767/1000], Loss:4.2915162339340895e-06\n",
      "Epoch[768/1000], Loss:4.947157322021667e-06\n",
      "Epoch[769/1000], Loss:5.344513283489505e-06\n",
      "Epoch[770/1000], Loss:4.718677246273728e-06\n",
      "Epoch[771/1000], Loss:8.771718057687394e-06\n",
      "Epoch[772/1000], Loss:4.6789427869953215e-06\n",
      "Epoch[773/1000], Loss:4.500130216911202e-06\n",
      "Epoch[774/1000], Loss:5.662410330842249e-06\n",
      "Epoch[775/1000], Loss:6.109441073931521e-06\n",
      "Epoch[776/1000], Loss:4.72860983791179e-06\n",
      "Epoch[777/1000], Loss:6.894220859976485e-06\n",
      "Epoch[778/1000], Loss:4.380923655844526e-06\n",
      "Epoch[779/1000], Loss:5.722014520870289e-06\n",
      "Epoch[780/1000], Loss:5.314720056048827e-06\n",
      "Epoch[781/1000], Loss:4.480260031414218e-06\n",
      "Epoch[782/1000], Loss:5.81140875510755e-06\n",
      "Epoch[783/1000], Loss:4.370987426227657e-06\n",
      "Epoch[784/1000], Loss:4.708740107162157e-06\n",
      "Epoch[785/1000], Loss:3.993494374299189e-06\n",
      "Epoch[786/1000], Loss:6.33792114967946e-06\n",
      "Epoch[787/1000], Loss:4.539868768915767e-06\n",
      "Epoch[788/1000], Loss:7.639270734216552e-06\n",
      "Epoch[789/1000], Loss:6.894224952702643e-06\n",
      "Epoch[790/1000], Loss:8.384268767258618e-06\n",
      "Epoch[791/1000], Loss:5.076303295936668e-06\n",
      "Epoch[792/1000], Loss:4.380921836855123e-06\n",
      "Epoch[793/1000], Loss:4.023293058708077e-06\n",
      "Epoch[794/1000], Loss:6.2087783589959145e-06\n",
      "Epoch[795/1000], Loss:7.6094743235444184e-06\n",
      "Epoch[796/1000], Loss:6.48692957838648e-06\n",
      "Epoch[797/1000], Loss:7.06310993336956e-06\n",
      "Epoch[798/1000], Loss:5.582934591075173e-06\n",
      "Epoch[799/1000], Loss:4.390852609503781e-06\n",
      "Epoch[800/1000], Loss:5.990229965391336e-06\n",
      "Epoch[801/1000], Loss:4.351117240730673e-06\n",
      "Epoch[802/1000], Loss:9.894259164866526e-06\n",
      "Epoch[803/1000], Loss:3.6557378280122066e-06\n",
      "Epoch[804/1000], Loss:3.983561327913776e-06\n",
      "Epoch[805/1000], Loss:5.344518285710365e-06\n",
      "Epoch[806/1000], Loss:4.847820946451975e-06\n",
      "Epoch[807/1000], Loss:5.215379587752977e-06\n",
      "Epoch[808/1000], Loss:3.039826651729527e-06\n",
      "Epoch[809/1000], Loss:4.351120423962129e-06\n",
      "Epoch[810/1000], Loss:3.6855390135315247e-06\n",
      "Epoch[811/1000], Loss:5.771680662292056e-06\n",
      "Epoch[812/1000], Loss:4.818017714569578e-06\n",
      "Epoch[813/1000], Loss:5.602798410109244e-06\n",
      "Epoch[814/1000], Loss:5.652476374962134e-06\n",
      "Epoch[815/1000], Loss:4.57960140920477e-06\n",
      "Epoch[816/1000], Loss:7.917394214018714e-06\n",
      "Epoch[817/1000], Loss:4.529930720309494e-06\n",
      "Epoch[818/1000], Loss:4.758406703331275e-06\n",
      "Epoch[819/1000], Loss:6.010076958773425e-06\n",
      "Epoch[820/1000], Loss:5.3842595661990345e-06\n",
      "Epoch[821/1000], Loss:5.821341346745612e-06\n",
      "Epoch[822/1000], Loss:4.639209691958968e-06\n",
      "Epoch[823/1000], Loss:4.212043222651118e-06\n",
      "Epoch[824/1000], Loss:6.337910690490389e-06\n",
      "Epoch[825/1000], Loss:4.619338142219931e-06\n",
      "Epoch[826/1000], Loss:4.380922746349825e-06\n",
      "Epoch[827/1000], Loss:7.917414222902153e-06\n",
      "Epoch[828/1000], Loss:5.523327217815677e-06\n",
      "Epoch[829/1000], Loss:4.261713911546394e-06\n",
      "Epoch[830/1000], Loss:3.8146811220940435e-06\n",
      "Epoch[831/1000], Loss:3.923956683138385e-06\n",
      "Epoch[832/1000], Loss:6.566403044416802e-06\n",
      "Epoch[833/1000], Loss:5.414056431618519e-06\n",
      "Epoch[834/1000], Loss:3.5067284898104845e-06\n",
      "Epoch[835/1000], Loss:5.165703896636842e-06\n",
      "Epoch[836/1000], Loss:4.202107902528951e-06\n",
      "Epoch[837/1000], Loss:4.291513050702633e-06\n",
      "Epoch[838/1000], Loss:4.361054834589595e-06\n",
      "Epoch[839/1000], Loss:3.5365276289667236e-06\n",
      "Epoch[840/1000], Loss:4.27164832217386e-06\n",
      "Epoch[841/1000], Loss:6.9935681494825985e-06\n",
      "Epoch[842/1000], Loss:4.986891326552723e-06\n",
      "Epoch[843/1000], Loss:4.122637619730085e-06\n",
      "Epoch[844/1000], Loss:8.05644322099397e-06\n",
      "Epoch[845/1000], Loss:4.559735771181295e-06\n",
      "Epoch[846/1000], Loss:4.549797722575022e-06\n",
      "Epoch[847/1000], Loss:4.818019988306332e-06\n",
      "Epoch[848/1000], Loss:5.364392109186156e-06\n",
      "Epoch[849/1000], Loss:7.1028375714377034e-06\n",
      "Epoch[850/1000], Loss:3.923955773643684e-06\n",
      "Epoch[851/1000], Loss:4.460394848138094e-06\n",
      "Epoch[852/1000], Loss:5.503466582013061e-06\n",
      "Epoch[853/1000], Loss:7.023361831670627e-06\n",
      "Epoch[854/1000], Loss:5.960425824014237e-06\n",
      "Epoch[855/1000], Loss:5.384252744988771e-06\n",
      "Epoch[856/1000], Loss:4.669008831115207e-06\n",
      "Epoch[857/1000], Loss:4.827937573281815e-06\n",
      "Epoch[858/1000], Loss:5.354448148864321e-06\n",
      "Epoch[859/1000], Loss:3.5762657262239372e-06\n",
      "Epoch[860/1000], Loss:4.34118419434526e-06\n",
      "Epoch[861/1000], Loss:4.00343151341076e-06\n",
      "Epoch[862/1000], Loss:4.321316737332381e-06\n",
      "Epoch[863/1000], Loss:4.490193987294333e-06\n",
      "Epoch[864/1000], Loss:5.076297384221107e-06\n",
      "Epoch[865/1000], Loss:3.963691142416792e-06\n",
      "Epoch[866/1000], Loss:7.847864253562875e-06\n",
      "Epoch[867/1000], Loss:4.808080575458007e-06\n",
      "Epoch[868/1000], Loss:4.738548796012765e-06\n",
      "Epoch[869/1000], Loss:4.599470685207052e-06\n",
      "Epoch[870/1000], Loss:4.440526936377864e-06\n",
      "Epoch[871/1000], Loss:4.1127027543552686e-06\n",
      "Epoch[872/1000], Loss:3.993490736320382e-06\n",
      "Epoch[873/1000], Loss:5.751819116994739e-06\n",
      "Epoch[874/1000], Loss:5.304782916937256e-06\n",
      "Epoch[875/1000], Loss:6.715411927871173e-06\n",
      "Epoch[876/1000], Loss:3.725276428667712e-06\n",
      "Epoch[877/1000], Loss:3.794811163970735e-06\n",
      "Epoch[878/1000], Loss:3.5762668630923145e-06\n",
      "Epoch[879/1000], Loss:5.761747161159292e-06\n",
      "Epoch[880/1000], Loss:4.241845090291463e-06\n",
      "Epoch[881/1000], Loss:3.864352493110346e-06\n",
      "Epoch[882/1000], Loss:6.476993348769611e-06\n",
      "Epoch[883/1000], Loss:4.51006553703337e-06\n",
      "Epoch[884/1000], Loss:3.6756064218934625e-06\n",
      "Epoch[885/1000], Loss:4.788218120665988e-06\n",
      "Epoch[886/1000], Loss:4.629271188605344e-06\n",
      "Epoch[887/1000], Loss:4.341186013334664e-06\n",
      "Epoch[888/1000], Loss:4.241847364028217e-06\n",
      "Epoch[889/1000], Loss:3.804749439950683e-06\n",
      "Epoch[890/1000], Loss:4.60940600532922e-06\n",
      "Epoch[891/1000], Loss:3.9835595089243725e-06\n",
      "Epoch[892/1000], Loss:4.1623729885031935e-06\n",
      "Epoch[893/1000], Loss:3.387517608643975e-06\n",
      "Epoch[894/1000], Loss:7.480323347408557e-06\n",
      "Epoch[895/1000], Loss:4.947156867274316e-06\n",
      "Epoch[896/1000], Loss:7.013426056801109e-06\n",
      "Epoch[897/1000], Loss:7.490251391573111e-06\n",
      "Epoch[898/1000], Loss:4.1425023482588585e-06\n",
      "Epoch[899/1000], Loss:3.337848283990752e-06\n",
      "Epoch[900/1000], Loss:5.15577130499878e-06\n",
      "Epoch[901/1000], Loss:5.1557758524722885e-06\n",
      "Epoch[902/1000], Loss:6.308124284259975e-06\n",
      "Epoch[903/1000], Loss:3.2683085464668693e-06\n",
      "Epoch[904/1000], Loss:5.543206953007029e-06\n",
      "Epoch[905/1000], Loss:3.2484451821801485e-06\n",
      "Epoch[906/1000], Loss:3.9438227759092115e-06\n",
      "Epoch[907/1000], Loss:9.09955724637257e-06\n",
      "Epoch[908/1000], Loss:4.559734861686593e-06\n",
      "Epoch[909/1000], Loss:4.380920927360421e-06\n",
      "Epoch[910/1000], Loss:4.877620540355565e-06\n",
      "Epoch[911/1000], Loss:3.824613031611079e-06\n",
      "Epoch[912/1000], Loss:5.533271178137511e-06\n",
      "Epoch[913/1000], Loss:3.8047492125770077e-06\n",
      "Epoch[914/1000], Loss:2.900751496781595e-06\n",
      "Epoch[915/1000], Loss:5.284905000735307e-06\n",
      "Epoch[916/1000], Loss:4.510065082286019e-06\n",
      "Epoch[917/1000], Loss:3.5365289932087762e-06\n",
      "Epoch[918/1000], Loss:4.529932539298898e-06\n",
      "Epoch[919/1000], Loss:2.463652663209359e-06\n",
      "Epoch[920/1000], Loss:3.834547896985896e-06\n",
      "Epoch[921/1000], Loss:5.384250925999368e-06\n",
      "Epoch[922/1000], Loss:5.930621682637138e-06\n",
      "Epoch[923/1000], Loss:5.622671324090334e-06\n",
      "Epoch[924/1000], Loss:3.95375855077873e-06\n",
      "Epoch[925/1000], Loss:3.407387112019933e-06\n",
      "Epoch[926/1000], Loss:3.983560418419074e-06\n",
      "Epoch[927/1000], Loss:5.602807959803613e-06\n",
      "Epoch[928/1000], Loss:4.311384145694319e-06\n",
      "Epoch[929/1000], Loss:4.579601863952121e-06\n",
      "Epoch[930/1000], Loss:4.023296696686884e-06\n",
      "Epoch[931/1000], Loss:3.37758615387429e-06\n",
      "Epoch[932/1000], Loss:4.092837571079144e-06\n",
      "Epoch[933/1000], Loss:4.033222467114683e-06\n",
      "Epoch[934/1000], Loss:6.397527158696903e-06\n",
      "Epoch[935/1000], Loss:4.639203780243406e-06\n",
      "Epoch[936/1000], Loss:4.510064627538668e-06\n",
      "Epoch[937/1000], Loss:3.2881787319638534e-06\n",
      "Epoch[938/1000], Loss:3.675608240882866e-06\n",
      "Epoch[939/1000], Loss:5.284913186187623e-06\n",
      "Epoch[940/1000], Loss:2.4735863917157985e-06\n",
      "Epoch[941/1000], Loss:3.2285734050674364e-06\n",
      "Epoch[942/1000], Loss:4.023298515676288e-06\n",
      "Epoch[943/1000], Loss:5.652478193951538e-06\n",
      "Epoch[944/1000], Loss:4.659075329982443e-06\n",
      "Epoch[945/1000], Loss:4.897486633126391e-06\n",
      "Epoch[946/1000], Loss:2.751740112216794e-06\n",
      "Epoch[947/1000], Loss:4.2617125473043416e-06\n",
      "Epoch[948/1000], Loss:3.6855390135315247e-06\n",
      "Epoch[949/1000], Loss:5.880951448489213e-06\n",
      "Epoch[950/1000], Loss:4.827951215702342e-06\n",
      "Epoch[951/1000], Loss:4.6690097406099085e-06\n",
      "Epoch[952/1000], Loss:7.470396667486057e-06\n",
      "Epoch[953/1000], Loss:4.947160050505772e-06\n",
      "Epoch[954/1000], Loss:6.814749667682918e-06\n",
      "Epoch[955/1000], Loss:3.943826413888019e-06\n",
      "Epoch[956/1000], Loss:3.5464643133309437e-06\n",
      "Epoch[957/1000], Loss:4.867687948717503e-06\n",
      "Epoch[958/1000], Loss:5.304782462189905e-06\n",
      "Epoch[959/1000], Loss:5.0763064791681245e-06\n",
      "Epoch[960/1000], Loss:3.188838491041679e-06\n",
      "Epoch[961/1000], Loss:4.877621449850267e-06\n",
      "Epoch[962/1000], Loss:3.6160026866127737e-06\n",
      "Epoch[963/1000], Loss:5.086236797069432e-06\n",
      "Epoch[964/1000], Loss:5.056434019934386e-06\n",
      "Epoch[965/1000], Loss:4.718679065263132e-06\n",
      "Epoch[966/1000], Loss:2.6126635930268094e-06\n",
      "Epoch[967/1000], Loss:5.453790436149575e-06\n",
      "Epoch[968/1000], Loss:7.192201792349806e-06\n",
      "Epoch[969/1000], Loss:2.8510814900073456e-06\n",
      "Epoch[970/1000], Loss:4.450458163773874e-06\n",
      "Epoch[971/1000], Loss:3.904089226125507e-06\n",
      "Epoch[972/1000], Loss:4.172308763372712e-06\n",
      "Epoch[973/1000], Loss:4.65907896796125e-06\n",
      "Epoch[974/1000], Loss:3.5365292205824517e-06\n",
      "Epoch[975/1000], Loss:4.122638529224787e-06\n",
      "Epoch[976/1000], Loss:5.125975349073997e-06\n",
      "Epoch[977/1000], Loss:5.0961721171916e-06\n",
      "Epoch[978/1000], Loss:5.096167569718091e-06\n",
      "Epoch[979/1000], Loss:6.715421477565542e-06\n",
      "Epoch[980/1000], Loss:4.370984697743552e-06\n",
      "Epoch[981/1000], Loss:3.029893605344114e-06\n",
      "Epoch[982/1000], Loss:4.698809789260849e-06\n",
      "Epoch[983/1000], Loss:2.5927956812665798e-06\n",
      "Epoch[984/1000], Loss:4.529929810814792e-06\n",
      "Epoch[985/1000], Loss:3.7749493912997423e-06\n",
      "Epoch[986/1000], Loss:6.010098786646267e-06\n",
      "Epoch[987/1000], Loss:4.60940100310836e-06\n",
      "Epoch[988/1000], Loss:3.586200591598754e-06\n",
      "Epoch[989/1000], Loss:3.993493010057136e-06\n",
      "Epoch[990/1000], Loss:4.380911832413403e-06\n",
      "Epoch[991/1000], Loss:5.841219262947561e-06\n",
      "Epoch[992/1000], Loss:3.983562692155829e-06\n",
      "Epoch[993/1000], Loss:3.923957592633087e-06\n",
      "Epoch[994/1000], Loss:5.036569291405613e-06\n",
      "Epoch[995/1000], Loss:3.3279145554843126e-06\n",
      "Epoch[996/1000], Loss:3.2981133699649945e-06\n",
      "Epoch[997/1000], Loss:4.0431664274365176e-06\n",
      "Epoch[998/1000], Loss:4.122638529224787e-06\n",
      "Epoch[999/1000], Loss:5.0663729780353606e-06\n",
      "Epoch[1000/1000], Loss:3.0497624265990453e-06\n"
     ]
    }
   ],
   "source": [
    "model = Transformer().cuda()\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(),lr = 1e-3, momentum=0.99)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len] [2,5]\n",
    "        dec_inputs: [batch_size, tgt_len] [2,6]\n",
    "        dec_outputs: [batch_size, tgt_len] [2,6]\n",
    "        '''\n",
    "        enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(),dec_outputs.cuda()\n",
    "        \n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        #outputs: [batch_size * tgt_len, tgt_vocab_size], dec_outputs: [batch_size, tgt_len]\n",
    "        loss = criterion(outputs,dec_outputs.view(-1))# 将dec_outputs展平成一维张量\n",
    "        #更新权重\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch[{epoch+1}/1000], Loss:{loss.item()}')\n",
    "torch.save(model,'MyPt.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(8, device='cuda:0')\n",
      "tensor([1, 2, 3, 5, 0], device='cuda:0') -> ['i', 'want', 'a', 'coke', '.']\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(8, device='cuda:0')\n",
      "tensor([1, 2, 3, 4, 0], device='cuda:0') -> ['i', 'want', 'a', 'beer', '.']\n"
     ]
    }
   ],
   "source": [
    "# 原文使用的是大小为4的beam search，这里为简单起见使用更简单的greedy贪心策略生成预测，不考虑候选，每一步选择概率最大的作为输出\n",
    "# 如果不使用greedy_decoder，那么我们之前实现的model只会进行一次预测得到['i']，并不会自回归，\n",
    "# 所以我们利用编写好的Encoder-Decoder来手动实现自回归（把上一次Decoder的输出作为下一次的输入，直到预测出终止符）\n",
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"enc_input: [1, seq_len] 对应一句话\"\"\"\n",
    "    enc_outputs = model.encoder(enc_input) # enc_outputs: [1, seq_len, 512]\n",
    "    # 生成一个1行0列的，和enc_inputs.data类型相同的空张量，待后续填充\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data) # .data避免影响梯度信息\n",
    "    next_symbol = start_symbol\n",
    "    flag = True\n",
    "    while flag:\n",
    "        # dec_input.detach() 创建 dec_input 的一个分离副本\n",
    "        # 生成了一个 只含有next_symbol的（1,1）的张量\n",
    "        # -1 表示在最后一个维度上进行拼接cat\n",
    "        # 这行代码的作用是将next_symbol拼接到dec_input中，作为新一轮decoder的输入\n",
    "        dec_input = torch.cat([dec_input.detach(), torch.tensor([[next_symbol]], dtype=enc_input.dtype).cuda()], -1) # dec_input: [1,当前词数]\n",
    "        dec_outputs = model.decoder(dec_input, enc_input, enc_outputs) # dec_outputs: [1, tgt_len, d_model]\n",
    "        projected = model.projection(dec_outputs) # projected: [1, 当前生成的tgt_len, tgt_vocab_size]\n",
    "        # max返回的是一个元组（最大值，最大值对应的索引），所以用[1]取到最大值对应的索引, 索引就是类别，即预测出的下一个词\n",
    "        # keepdim为False会导致减少一维\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1] # prob: [1],\n",
    "        # prob是一个一维的列表，包含目前为止依次生成的词的索引，最后一个是新生成的（即下一个词的类别）\n",
    "        # 因为注意力是依照前面的词算出来的，所以后生成的不会改变之前生成的\n",
    "        next_symbol = prob.data[-1]\n",
    "        if next_symbol == tgt_vocab['.']:\n",
    "            flag = False\n",
    "        print(next_symbol)\n",
    "    return dec_input  # dec_input: [1,tgt_len]\n",
    " \n",
    " \n",
    "# 测试\n",
    "model = torch.load('MyPT.pth', weights_only=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 手动从loader中取一个batch的数据\n",
    "    enc_inputs, _, _ = next(iter(loader))\n",
    "    enc_inputs = enc_inputs.cuda()\n",
    "    for i in range(len(enc_inputs)):\n",
    "        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab['S'])\n",
    "        predict  = model(enc_inputs[i].view(1, -1), greedy_dec_input) # predict: [batch_size * tgt_len, tgt_vocab_size]\n",
    "        predict = predict.data.max(dim=-1, keepdim=False)[1]\n",
    "        '''greedy_dec_input是基于贪婪策略生成的，而贪婪解码的输出是基于当前时间步生成的假设的输出。\n",
    "        这意味着它可能不是最优的输出，因为它仅考虑了每个时间步的最有可能的单词，而没有考虑全局上下文。\n",
    "        因此，为了获得更好的性能评估，通常会将整个输入序列和之前的假设输出序列传递给模型，\n",
    "        以考虑全局上下文并允许模型更准确地生成输出\n",
    "        '''\n",
    "        print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())  # 应该返回 True\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))  # 输出你的 GPU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
