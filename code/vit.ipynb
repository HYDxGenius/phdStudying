{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 调整image size\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "img = Image.open(\"vitpic/1.png\").convert(\"RGB\")\n",
    "\n",
    "x = transform(img)\n",
    "x = x.unsqueeze(0)  # add batch dim\n",
    "print(x.shape)  # torch.Size([1, 3, 224, 224])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步把image分割为pathces，然后将其flatten, 用einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size=16  # pixels\n",
    "patches=rearrange(x,'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1=patch_size,s2=patch_size)\n",
    "print(patches.shape) # (batch, patch数量（224/16）^2, 每一个patch的维度（16x16x3）)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # ✅ 使用 Conv2D 进行 Patch Embedding\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "\n",
    "        # ✅ Class Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "\n",
    "        # ✅ 位置编码\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.positions = nn.Parameter(torch.randn(1, 197, emb_size))  # ✅ 确保 shape = [1, 197, 768]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.projection(x)  # shape: [B, 196, 768]\n",
    "\n",
    "        # ✅ 复制 CLS Token\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # shape: [B, 1, 768]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # shape: [B, 197, 768]\n",
    "\n",
    "        # ✅ 添加 Position Encoding\n",
    "        x = x + self.positions.expand(x.shape[0], -1, -1)  # ✅ 让 positions 适应 batch 维度\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer 在vit中only encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "# patches_embedded=PatchEmbedding()(x)\n",
    "#print(MultiHeadAttention()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "\n",
    "\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接用调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "#         super().__init__()\n",
    "#         # 利用 PyTorch 内置的 MultiheadAttention 实现多头注意力\n",
    "#         self.attention = nn.MultiheadAttention(\n",
    "#             embed_dim=emb_size,\n",
    "#             num_heads=num_heads,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True  # 确保输入输出形状为 (batch, seq, emb)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "#         # 对于 nn.MultiheadAttention, query, key, value 一般均为 x\n",
    "#         # 如果提供 mask，则传递给 attn_mask 参数\n",
    "#         att_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "#         return att_output\n",
    "\n",
    "\n",
    "\n",
    "# # 测试\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape)  # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self,x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "# class ResidualAdd(nn.Module):\n",
    "#     def __init__(self, layer):\n",
    "#         super().__init__()\n",
    "#         self.layer = layer  # 任何传入的计算层（如 MHA 或 FFN）\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.layer(x)  # 直接残差连接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size), #dmodel dff\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "# class FeedForwardBlock(nn.Module):\n",
    "#     def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(emb_size, expansion * emb_size)  # d_model -> d_ff\n",
    "#         self.act = nn.GELU()  # 激活函数\n",
    "#         self.dropout = nn.Dropout(drop_p)\n",
    "#         self.fc2 = nn.Linear(expansion * emb_size, emb_size)  # d_ff -> d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.dropout(self.act(self.fc1(x))))  # 线性 -> GELU -> Dropout -> 线性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Block组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoderBlock(nn.Sequential):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, drop_p: float = 0., forward_expansion: int = 4):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(emb_size)\n",
    "#         self.attn = ResidualAdd(MultiHeadAttention(emb_size, num_heads=num_heads))\n",
    "#         self.dropout1 = nn.Dropout(drop_p)\n",
    "\n",
    "#         self.norm2 = nn.LayerNorm(emb_size)\n",
    "#         self.ffn = ResidualAdd(FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=drop_p))\n",
    "#         self.dropout2 = nn.Dropout(drop_p)\n",
    "# patches_embedded = PatchEmbedding()(x)\n",
    "    # def forward(self, x):\n",
    "    #     x = self.attn(self.norm1(x))  # MHA + 残差\n",
    "    #     x = self.dropout1(x)\n",
    "    #     x = self.ffn(self.norm2(x))  # FFN + 残差\n",
    "    #     x = self.dropout2(x)\n",
    "    #     return x\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "    ))\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x)  # 依次通过每个 Transformer Encoder Block\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类头\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "🔄 加载最新的 Checkpoint: training_dir/vit_epoch_1.pth\n",
      "✅ Checkpoint training_dir/vit_epoch_1.pth 加载成功，从 Epoch 2 继续训练！\n",
      "Epoch 2 开始训练...\n",
      "Train Epoch: 2 [0/50000] Loss: 1.744868\n",
      "Train Epoch: 2 [160/50000] Loss: 1.569748\n",
      "Train Epoch: 2 [320/50000] Loss: 1.814432\n",
      "Train Epoch: 2 [480/50000] Loss: 1.787008\n",
      "Train Epoch: 2 [640/50000] Loss: 2.029443\n",
      "Train Epoch: 2 [800/50000] Loss: 2.434287\n",
      "Train Epoch: 2 [960/50000] Loss: 1.763276\n",
      "Train Epoch: 2 [1120/50000] Loss: 1.773651\n",
      "Train Epoch: 2 [1280/50000] Loss: 2.174492\n",
      "Train Epoch: 2 [1440/50000] Loss: 2.105923\n",
      "Train Epoch: 2 [1600/50000] Loss: 2.190749\n",
      "Train Epoch: 2 [1760/50000] Loss: 2.200714\n",
      "Train Epoch: 2 [1920/50000] Loss: 1.745773\n",
      "Train Epoch: 2 [2080/50000] Loss: 2.010264\n",
      "Train Epoch: 2 [2240/50000] Loss: 1.897064\n",
      "Train Epoch: 2 [2400/50000] Loss: 2.396065\n",
      "Train Epoch: 2 [2560/50000] Loss: 2.297965\n",
      "Train Epoch: 2 [2720/50000] Loss: 2.259289\n",
      "Train Epoch: 2 [2880/50000] Loss: 1.870618\n",
      "Train Epoch: 2 [3040/50000] Loss: 1.954506\n",
      "Train Epoch: 2 [3200/50000] Loss: 1.856021\n",
      "Train Epoch: 2 [3360/50000] Loss: 2.284893\n",
      "Train Epoch: 2 [3520/50000] Loss: 2.232515\n",
      "Train Epoch: 2 [3680/50000] Loss: 1.982586\n",
      "Train Epoch: 2 [3840/50000] Loss: 1.954264\n",
      "Train Epoch: 2 [4000/50000] Loss: 1.757510\n",
      "Train Epoch: 2 [4160/50000] Loss: 2.107173\n",
      "Train Epoch: 2 [4320/50000] Loss: 1.767241\n",
      "Train Epoch: 2 [4480/50000] Loss: 2.168341\n",
      "Train Epoch: 2 [4640/50000] Loss: 2.236884\n",
      "Train Epoch: 2 [4800/50000] Loss: 1.952621\n",
      "Train Epoch: 2 [4960/50000] Loss: 1.751391\n",
      "Train Epoch: 2 [5120/50000] Loss: 2.307239\n",
      "Train Epoch: 2 [5280/50000] Loss: 2.235882\n",
      "Train Epoch: 2 [5440/50000] Loss: 1.893474\n",
      "Train Epoch: 2 [5600/50000] Loss: 1.913245\n",
      "Train Epoch: 2 [5760/50000] Loss: 1.952348\n",
      "Train Epoch: 2 [5920/50000] Loss: 2.210131\n",
      "Train Epoch: 2 [6080/50000] Loss: 2.116408\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# ✅ 训练目录\n",
    "checkpoint_dir = Path(\"training_dir\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ✅ 设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ 预处理（数据增强）\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # 防止 Patch 过度裁剪\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ✅ 加载 CIFAR-10 数据集\n",
    "train_dataset = datasets.CIFAR10(root=checkpoint_dir, train=True, download=False, transform=transform)\n",
    "# 修改后（测试集），和训练集保持一致\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=checkpoint_dir, \n",
    "    train=False, \n",
    "    download=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # 这里可以加上与训练集一致的 Normalize\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# ✅ **实例化模型**\n",
    "model = ViT(\n",
    "    in_channels=3,\n",
    "    patch_size=16,  # ✅ 适应 CIFAR-10\n",
    "    emb_size=768,\n",
    "    img_size=224,\n",
    "    depth=12,\n",
    "    n_classes=10\n",
    ").to(device)\n",
    "\n",
    "# ✅ **优化器**\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# ✅ **学习率调度器**\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "# ✅ 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ✅ **检查是否有已训练的 Checkpoint**\n",
    "checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")])\n",
    "latest_checkpoint = checkpoint_dir / checkpoint_files[-1] if checkpoint_files else None\n",
    "\n",
    "# ✅ **如果存在 Checkpoint，则加载**\n",
    "start_epoch = 1\n",
    "if latest_checkpoint:\n",
    "    print(f\"🔄 加载最新的 Checkpoint: {latest_checkpoint}\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)  # ✅ 忽略 shape 不匹配\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # 从下一个 epoch 开始训练\n",
    "    print(f\"✅ Checkpoint {latest_checkpoint} 加载成功，从 Epoch {start_epoch} 继续训练！\")\n",
    "\n",
    "\n",
    "# ✅ 训练过程\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch} 开始训练...\")\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ✅ 每 10 个 Batch 打印一次 Loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "    scheduler.step()  # ✅ 更新学习率\n",
    "\n",
    "    # ✅ 训练完成后，保存 Checkpoint\n",
    "    checkpoint_path = checkpoint_dir / f\"vit_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"✅ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "# ✅ 测试过程\n",
    "def test(model, device, test_loader, criterion):\n",
    "    print(\"testing\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # if data.shape[1:] != (3, 224, 224):  # ✅ 检查数据 shape\n",
    "            #     raise ValueError(f\"Test batch shape mismatch: {data.shape}\")\n",
    "\n",
    "            output = model(data)  # ✅ 确保输入一致\n",
    "\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n\")\n",
    "\n",
    "\n",
    "# ✅ 训练循环（扩展到 50 Epoch）\n",
    "num_epochs = 50\n",
    "for epoch in range(start_epoch, num_epochs + 1):  # ✅ 从 start_epoch 开始\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
