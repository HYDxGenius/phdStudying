{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 调整image size\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "img = Image.open(\"vitpic/1.png\").convert(\"RGB\")\n",
    "\n",
    "x = transform(img)\n",
    "x = x.unsqueeze(0)  # add batch dim\n",
    "print(x.shape)  # torch.Size([1, 3, 224, 224])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步把image分割为pathces，然后将其flatten, 用einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size=16  # pixels\n",
    "patches=rearrange(x,'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1=patch_size,s2=patch_size)\n",
    "print(patches.shape) # (batch, patch数量（224/16）^2, 每一个patch的维度（16x16x3）)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int =3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,emb_size,kernel_size=patch_size, stride=patch_size),\n",
    "            #cnn后成（batch,emb_size,new_h = h/patch_size, new_w = w/patch_size)\n",
    "            Rearrange('b e (h) (w) -> b (h w) e' ),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
    "        #生成一个class token 1,1,e 参数化 [1, 1, 768]\n",
    "\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**2 +1, emb_size ))\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        # print(\"输入 x 形状:\", x.shape)  # 打印输入形状\n",
    "        # x1 = self.projection[0](x)  # 只经过 Conv2d\n",
    "        # print(\"Conv2d 之后 x 形状:\", x1.shape)  # 打印 Conv2d 之后的形状\n",
    "        # x1 = self.projection[1](x1)  # 经过 Rearrange\n",
    "        # print(\"Rearrange 之后 x 形状:\", x1.shape)  # 打印最终形状\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        b, _, _, _ = x.shape#就是b=x.shape[0]\n",
    "        # print(b)\n",
    "\n",
    "\n",
    "        x=self.projection(x)\n",
    "        \n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)# [batch_size, 1, 768]\n",
    "        \n",
    "        #cls_token： 1（不管是多少）,1, e -> batch_size , 1, e\n",
    "        x=torch.cat([cls_tokens,x],dim=1)\n",
    "        # print(\"加了class token 之后 x 形状:\", x.shape) \n",
    "\n",
    "        x +=self.positions\n",
    "        # print(\"加了Position token 之后 x 形状:\", x.shape)  \n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "# patch_embedding = PatchEmbedding()\n",
    "# patches = patch_embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer 在vit中only encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "#         super().__init__()\n",
    "#         self.emb_size = emb_size\n",
    "#         self.num_heads = num_heads\n",
    "#         # fuse the queries, keys and values in one matrix\n",
    "#         self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "#         self.att_drop = nn.Dropout(dropout)\n",
    "#         self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "#     def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "#         # split keys, queries and values in num_heads\n",
    "#         qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "#         queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "#         # sum up over the last axis\n",
    "#         energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "#         if mask is not None:\n",
    "#             fill_value = torch.finfo(torch.float32).min\n",
    "#             energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "#         scaling = self.emb_size ** (1 / 2)\n",
    "#         att = F.softmax(energy, dim=-1) / scaling\n",
    "#         att = self.att_drop(att)\n",
    "#         # sum up over the third axis\n",
    "#         out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "#         out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "#         out = self.projection(out)\n",
    "#         return out\n",
    "\n",
    "# patches_embedded=PatchEmbedding()(x)\n",
    "#print(MultiHeadAttention()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "\n",
    "\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接用调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        # 利用 PyTorch 内置的 MultiheadAttention 实现多头注意力\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=emb_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # 确保输入输出形状为 (batch, seq, emb)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # 对于 nn.MultiheadAttention, query, key, value 一般均为 x\n",
    "        # 如果提供 mask，则传递给 attn_mask 参数\n",
    "        att_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        return att_output\n",
    "\n",
    "\n",
    "\n",
    "# # 测试\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape)  # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self,x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "# class ResidualAdd(nn.Module):\n",
    "#     def __init__(self, layer):\n",
    "#         super().__init__()\n",
    "#         self.layer = layer  # 任何传入的计算层（如 MHA 或 FFN）\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.layer(x)  # 直接残差连接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size), #dmodel dff\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "# class FeedForwardBlock(nn.Module):\n",
    "#     def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(emb_size, expansion * emb_size)  # d_model -> d_ff\n",
    "#         self.act = nn.GELU()  # 激活函数\n",
    "#         self.dropout = nn.Dropout(drop_p)\n",
    "#         self.fc2 = nn.Linear(expansion * emb_size, emb_size)  # d_ff -> d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.dropout(self.act(self.fc1(x))))  # 线性 -> GELU -> Dropout -> 线性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Block组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoderBlock(nn.Sequential):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, drop_p: float = 0., forward_expansion: int = 4):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(emb_size)\n",
    "#         self.attn = ResidualAdd(MultiHeadAttention(emb_size, num_heads=num_heads))\n",
    "#         self.dropout1 = nn.Dropout(drop_p)\n",
    "\n",
    "#         self.norm2 = nn.LayerNorm(emb_size)\n",
    "#         self.ffn = ResidualAdd(FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=drop_p))\n",
    "#         self.dropout2 = nn.Dropout(drop_p)\n",
    "# patches_embedded = PatchEmbedding()(x)\n",
    "    # def forward(self, x):\n",
    "    #     x = self.attn(self.norm1(x))  # MHA + 残差\n",
    "    #     x = self.dropout1(x)\n",
    "    #     x = self.ffn(self.norm2(x))  # FFN + 残差\n",
    "    #     x = self.dropout2(x)\n",
    "    #     return x\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "    ))\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x)  # 依次通过每个 Transformer Encoder Block\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类头\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-5  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-6             [-1, 197, 768]               0\n",
      "           Dropout-7             [-1, 197, 768]               0\n",
      "       ResidualAdd-8             [-1, 197, 768]               0\n",
      "         LayerNorm-9             [-1, 197, 768]           1,536\n",
      "           Linear-10            [-1, 197, 3072]       2,362,368\n",
      "             GELU-11            [-1, 197, 3072]               0\n",
      "          Dropout-12            [-1, 197, 3072]               0\n",
      "           Linear-13             [-1, 197, 768]       2,360,064\n",
      "          Dropout-14             [-1, 197, 768]               0\n",
      "      ResidualAdd-15             [-1, 197, 768]               0\n",
      "        LayerNorm-16             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-17  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-18             [-1, 197, 768]               0\n",
      "          Dropout-19             [-1, 197, 768]               0\n",
      "      ResidualAdd-20             [-1, 197, 768]               0\n",
      "        LayerNorm-21             [-1, 197, 768]           1,536\n",
      "           Linear-22            [-1, 197, 3072]       2,362,368\n",
      "             GELU-23            [-1, 197, 3072]               0\n",
      "          Dropout-24            [-1, 197, 3072]               0\n",
      "           Linear-25             [-1, 197, 768]       2,360,064\n",
      "          Dropout-26             [-1, 197, 768]               0\n",
      "      ResidualAdd-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-29  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-30             [-1, 197, 768]               0\n",
      "          Dropout-31             [-1, 197, 768]               0\n",
      "      ResidualAdd-32             [-1, 197, 768]               0\n",
      "        LayerNorm-33             [-1, 197, 768]           1,536\n",
      "           Linear-34            [-1, 197, 3072]       2,362,368\n",
      "             GELU-35            [-1, 197, 3072]               0\n",
      "          Dropout-36            [-1, 197, 3072]               0\n",
      "           Linear-37             [-1, 197, 768]       2,360,064\n",
      "          Dropout-38             [-1, 197, 768]               0\n",
      "      ResidualAdd-39             [-1, 197, 768]               0\n",
      "        LayerNorm-40             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-41  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-42             [-1, 197, 768]               0\n",
      "          Dropout-43             [-1, 197, 768]               0\n",
      "      ResidualAdd-44             [-1, 197, 768]               0\n",
      "        LayerNorm-45             [-1, 197, 768]           1,536\n",
      "           Linear-46            [-1, 197, 3072]       2,362,368\n",
      "             GELU-47            [-1, 197, 3072]               0\n",
      "          Dropout-48            [-1, 197, 3072]               0\n",
      "           Linear-49             [-1, 197, 768]       2,360,064\n",
      "          Dropout-50             [-1, 197, 768]               0\n",
      "      ResidualAdd-51             [-1, 197, 768]               0\n",
      "        LayerNorm-52             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-53  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-54             [-1, 197, 768]               0\n",
      "          Dropout-55             [-1, 197, 768]               0\n",
      "      ResidualAdd-56             [-1, 197, 768]               0\n",
      "        LayerNorm-57             [-1, 197, 768]           1,536\n",
      "           Linear-58            [-1, 197, 3072]       2,362,368\n",
      "             GELU-59            [-1, 197, 3072]               0\n",
      "          Dropout-60            [-1, 197, 3072]               0\n",
      "           Linear-61             [-1, 197, 768]       2,360,064\n",
      "          Dropout-62             [-1, 197, 768]               0\n",
      "      ResidualAdd-63             [-1, 197, 768]               0\n",
      "        LayerNorm-64             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-65  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-66             [-1, 197, 768]               0\n",
      "          Dropout-67             [-1, 197, 768]               0\n",
      "      ResidualAdd-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70            [-1, 197, 3072]       2,362,368\n",
      "             GELU-71            [-1, 197, 3072]               0\n",
      "          Dropout-72            [-1, 197, 3072]               0\n",
      "           Linear-73             [-1, 197, 768]       2,360,064\n",
      "          Dropout-74             [-1, 197, 768]               0\n",
      "      ResidualAdd-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-77  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-89  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-90             [-1, 197, 768]               0\n",
      "          Dropout-91             [-1, 197, 768]               0\n",
      "      ResidualAdd-92             [-1, 197, 768]               0\n",
      "        LayerNorm-93             [-1, 197, 768]           1,536\n",
      "           Linear-94            [-1, 197, 3072]       2,362,368\n",
      "             GELU-95            [-1, 197, 3072]               0\n",
      "          Dropout-96            [-1, 197, 3072]               0\n",
      "           Linear-97             [-1, 197, 768]       2,360,064\n",
      "          Dropout-98             [-1, 197, 768]               0\n",
      "      ResidualAdd-99             [-1, 197, 768]               0\n",
      "       LayerNorm-100             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-101  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-102             [-1, 197, 768]               0\n",
      "         Dropout-103             [-1, 197, 768]               0\n",
      "     ResidualAdd-104             [-1, 197, 768]               0\n",
      "       LayerNorm-105             [-1, 197, 768]           1,536\n",
      "          Linear-106            [-1, 197, 3072]       2,362,368\n",
      "            GELU-107            [-1, 197, 3072]               0\n",
      "         Dropout-108            [-1, 197, 3072]               0\n",
      "          Linear-109             [-1, 197, 768]       2,360,064\n",
      "         Dropout-110             [-1, 197, 768]               0\n",
      "     ResidualAdd-111             [-1, 197, 768]               0\n",
      "       LayerNorm-112             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-113  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-114             [-1, 197, 768]               0\n",
      "         Dropout-115             [-1, 197, 768]               0\n",
      "     ResidualAdd-116             [-1, 197, 768]               0\n",
      "       LayerNorm-117             [-1, 197, 768]           1,536\n",
      "          Linear-118            [-1, 197, 3072]       2,362,368\n",
      "            GELU-119            [-1, 197, 3072]               0\n",
      "         Dropout-120            [-1, 197, 3072]               0\n",
      "          Linear-121             [-1, 197, 768]       2,360,064\n",
      "         Dropout-122             [-1, 197, 768]               0\n",
      "     ResidualAdd-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-125  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-126             [-1, 197, 768]               0\n",
      "         Dropout-127             [-1, 197, 768]               0\n",
      "     ResidualAdd-128             [-1, 197, 768]               0\n",
      "       LayerNorm-129             [-1, 197, 768]           1,536\n",
      "          Linear-130            [-1, 197, 3072]       2,362,368\n",
      "            GELU-131            [-1, 197, 3072]               0\n",
      "         Dropout-132            [-1, 197, 3072]               0\n",
      "          Linear-133             [-1, 197, 768]       2,360,064\n",
      "         Dropout-134             [-1, 197, 768]               0\n",
      "     ResidualAdd-135             [-1, 197, 768]               0\n",
      "       LayerNorm-136             [-1, 197, 768]           1,536\n",
      "MultiheadAttention-137  [[-1, 197, 768], [-1, 197, 197]]               0\n",
      "MultiHeadAttention-138             [-1, 197, 768]               0\n",
      "         Dropout-139             [-1, 197, 768]               0\n",
      "     ResidualAdd-140             [-1, 197, 768]               0\n",
      "       LayerNorm-141             [-1, 197, 768]           1,536\n",
      "          Linear-142            [-1, 197, 3072]       2,362,368\n",
      "            GELU-143            [-1, 197, 3072]               0\n",
      "         Dropout-144            [-1, 197, 3072]               0\n",
      "          Linear-145             [-1, 197, 768]       2,360,064\n",
      "         Dropout-146             [-1, 197, 768]               0\n",
      "     ResidualAdd-147             [-1, 197, 768]               0\n",
      "          Reduce-148                  [-1, 768]               0\n",
      "       LayerNorm-149                  [-1, 768]           1,536\n",
      "          Linear-150                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 58,067,176\n",
      "Trainable params: 58,067,176\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 537284.79\n",
      "Params size (MB): 221.51\n",
      "Estimated Total Size (MB): 537506.87\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "print(summary(ViT(), (3, 224, 224), device='cpu'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
