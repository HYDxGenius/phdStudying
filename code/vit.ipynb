{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 调整image size\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "img = Image.open(\"vitpic/1.png\").convert(\"RGB\")\n",
    "\n",
    "x = transform(img)\n",
    "x = x.unsqueeze(0)  # add batch dim\n",
    "print(x.shape)  # torch.Size([1, 3, 224, 224])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步把image分割为pathces，然后将其flatten, 用einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size=16  # pixels\n",
    "patches=rearrange(x,'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1=patch_size,s2=patch_size)\n",
    "print(patches.shape) # (batch, patch数量（224/16）^2, 每一个patch的维度（16x16x3）)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int =3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,emb_size,kernel_size=patch_size, stride=patch_size),\n",
    "            #cnn后成（batch,emb_size,new_h = h/patch_size, new_w = w/patch_size)\n",
    "            Rearrange('b e (h) (w) -> b (h w) e' ),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
    "        #生成一个class token 1,1,e 参数化 [1, 1, 768]\n",
    "\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**2 +1, emb_size ))\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        # print(\"输入 x 形状:\", x.shape)  # 打印输入形状\n",
    "        # x1 = self.projection[0](x)  # 只经过 Conv2d\n",
    "        # print(\"Conv2d 之后 x 形状:\", x1.shape)  # 打印 Conv2d 之后的形状\n",
    "        # x1 = self.projection[1](x1)  # 经过 Rearrange\n",
    "        # print(\"Rearrange 之后 x 形状:\", x1.shape)  # 打印最终形状\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        b, _, _, _ = x.shape#就是b=x.shape[0]\n",
    "        # print(b)\n",
    "\n",
    "\n",
    "        x=self.projection(x)\n",
    "        \n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)# [batch_size, 1, 768]\n",
    "        \n",
    "        #cls_token： 1（不管是多少）,1, e -> batch_size , 1, e\n",
    "        x=torch.cat([cls_tokens,x],dim=1)\n",
    "        # print(\"加了class token 之后 x 形状:\", x.shape) \n",
    "\n",
    "        x +=self.positions\n",
    "        # print(\"加了Position token 之后 x 形状:\", x.shape)  \n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "# patch_embedding = PatchEmbedding()\n",
    "# patches = patch_embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer 在vit中only encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "# patches_embedded=PatchEmbedding()(x)\n",
    "#print(MultiHeadAttention()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "\n",
    "\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接用调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "#         super().__init__()\n",
    "#         # 利用 PyTorch 内置的 MultiheadAttention 实现多头注意力\n",
    "#         self.attention = nn.MultiheadAttention(\n",
    "#             embed_dim=emb_size,\n",
    "#             num_heads=num_heads,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True  # 确保输入输出形状为 (batch, seq, emb)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "#         # 对于 nn.MultiheadAttention, query, key, value 一般均为 x\n",
    "#         # 如果提供 mask，则传递给 attn_mask 参数\n",
    "#         att_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "#         return att_output\n",
    "\n",
    "\n",
    "\n",
    "# # 测试\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape)  # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self,x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "# class ResidualAdd(nn.Module):\n",
    "#     def __init__(self, layer):\n",
    "#         super().__init__()\n",
    "#         self.layer = layer  # 任何传入的计算层（如 MHA 或 FFN）\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.layer(x)  # 直接残差连接\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size), #dmodel dff\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "# class FeedForwardBlock(nn.Module):\n",
    "#     def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(emb_size, expansion * emb_size)  # d_model -> d_ff\n",
    "#         self.act = nn.GELU()  # 激活函数\n",
    "#         self.dropout = nn.Dropout(drop_p)\n",
    "#         self.fc2 = nn.Linear(expansion * emb_size, emb_size)  # d_ff -> d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.dropout(self.act(self.fc1(x))))  # 线性 -> GELU -> Dropout -> 线性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Block组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoderBlock(nn.Sequential):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, drop_p: float = 0., forward_expansion: int = 4):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(emb_size)\n",
    "#         self.attn = ResidualAdd(MultiHeadAttention(emb_size, num_heads=num_heads))\n",
    "#         self.dropout1 = nn.Dropout(drop_p)\n",
    "\n",
    "#         self.norm2 = nn.LayerNorm(emb_size)\n",
    "#         self.ffn = ResidualAdd(FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=drop_p))\n",
    "#         self.dropout2 = nn.Dropout(drop_p)\n",
    "# patches_embedded = PatchEmbedding()(x)\n",
    "    # def forward(self, x):\n",
    "    #     x = self.attn(self.norm1(x))  # MHA + 残差\n",
    "    #     x = self.dropout1(x)\n",
    "    #     x = self.ffn(self.norm2(x))  # FFN + 残差\n",
    "    #     x = self.dropout2(x)\n",
    "    #     return x\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "    ))\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x)  # 依次通过每个 Transformer Encoder Block\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类头\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 694.56\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "print(summary(ViT(), (3, 224, 224), device='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train Epoch: 1 [0/50000] Loss: 2.737005\n",
      "Train Epoch: 1 [320/50000] Loss: 3.748559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     test(model, device, test_loader, criterion)\n",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 42\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# 前向传播：模型输出 logits，直接进入分类头\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 假设之前定义的 ViT 模型已经导入，例如：\n",
    "# from vit import ViT\n",
    "\n",
    "# 数据预处理：调整图像尺寸和归一化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 因为ViT的输入尺寸通常为224x224\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集（这里以CIFAR-10为例）\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 实例化模型（注意n_classes需要与数据集类别数一致，CIFAR-10有10个类别）\n",
    "model = ViT(\n",
    "    in_channels=3,\n",
    "    patch_size=16,\n",
    "    emb_size=768,\n",
    "    img_size=224,\n",
    "    depth=12,\n",
    "    n_classes=10\n",
    ").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 定义训练过程\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    print(f\"Using device: {device}\")\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播：模型输出 logits，直接进入分类头\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "# 定义验证过程\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n\")\n",
    "\n",
    "# 主训练循环\n",
    "device = torch.device(\"cuda\")\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
