{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# è°ƒæ•´image size\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "img = Image.open(\"vitpic/1.png\").convert(\"RGB\")\n",
    "\n",
    "x = transform(img)\n",
    "x = x.unsqueeze(0)  # add batch dim\n",
    "print(x.shape)  # torch.Size([1, 3, 224, 224])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¬¬ä¸€æ­¥æŠŠimageåˆ†å‰²ä¸ºpathcesï¼Œç„¶åå°†å…¶flatten, ç”¨einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size=16  # pixels\n",
    "patches=rearrange(x,'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1=patch_size,s2=patch_size)\n",
    "print(patches.shape) # (batch, patchæ•°é‡ï¼ˆ224/16ï¼‰^2, æ¯ä¸€ä¸ªpatchçš„ç»´åº¦ï¼ˆ16x16x3ï¼‰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # âœ… ä½¿ç”¨ Conv2D è¿›è¡Œ Patch Embedding\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "\n",
    "        # âœ… Class Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "\n",
    "        # âœ… ä½ç½®ç¼–ç \n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.positions = nn.Parameter(torch.randn(1, 197, emb_size))  # âœ… ç¡®ä¿ shape = [1, 197, 768]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.projection(x)  # shape: [B, 196, 768]\n",
    "\n",
    "        # âœ… å¤åˆ¶ CLS Token\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # shape: [B, 1, 768]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # shape: [B, 197, 768]\n",
    "\n",
    "        # âœ… æ·»åŠ  Position Encoding\n",
    "        x = x + self.positions.expand(x.shape[0], -1, -1)  # âœ… è®© positions é€‚åº” batch ç»´åº¦\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer åœ¨vitä¸­only encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "# patches_embedded=PatchEmbedding()(x)\n",
    "#print(MultiHeadAttention()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "\n",
    "\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç›´æ¥ç”¨è°ƒåº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "#         super().__init__()\n",
    "#         # åˆ©ç”¨ PyTorch å†…ç½®çš„ MultiheadAttention å®ç°å¤šå¤´æ³¨æ„åŠ›\n",
    "#         self.attention = nn.MultiheadAttention(\n",
    "#             embed_dim=emb_size,\n",
    "#             num_heads=num_heads,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True  # ç¡®ä¿è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸º (batch, seq, emb)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "#         # å¯¹äº nn.MultiheadAttention, query, key, value ä¸€èˆ¬å‡ä¸º x\n",
    "#         # å¦‚æœæä¾› maskï¼Œåˆ™ä¼ é€’ç»™ attn_mask å‚æ•°\n",
    "#         att_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "#         return att_output\n",
    "\n",
    "\n",
    "\n",
    "# # æµ‹è¯•\n",
    "# patches_embedded = PatchEmbedding()(x)  # x: [batch_size, 3, 224, 224] -> [1, 197, 768]\n",
    "# mha = MultiHeadAttention()\n",
    "# print(mha(patches_embedded).shape)  # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self,x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "# class ResidualAdd(nn.Module):\n",
    "#     def __init__(self, layer):\n",
    "#         super().__init__()\n",
    "#         self.layer = layer  # ä»»ä½•ä¼ å…¥çš„è®¡ç®—å±‚ï¼ˆå¦‚ MHA æˆ– FFNï¼‰\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.layer(x)  # ç›´æ¥æ®‹å·®è¿æ¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size), #dmodel dff\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "# class FeedForwardBlock(nn.Module):\n",
    "#     def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(emb_size, expansion * emb_size)  # d_model -> d_ff\n",
    "#         self.act = nn.GELU()  # æ¿€æ´»å‡½æ•°\n",
    "#         self.dropout = nn.Dropout(drop_p)\n",
    "#         self.fc2 = nn.Linear(expansion * emb_size, emb_size)  # d_ff -> d_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.dropout(self.act(self.fc1(x))))  # çº¿æ€§ -> GELU -> Dropout -> çº¿æ€§\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Blockç»„åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoderBlock(nn.Sequential):\n",
    "#     def __init__(self, emb_size: int = 768, num_heads: int = 8, drop_p: float = 0., forward_expansion: int = 4):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(emb_size)\n",
    "#         self.attn = ResidualAdd(MultiHeadAttention(emb_size, num_heads=num_heads))\n",
    "#         self.dropout1 = nn.Dropout(drop_p)\n",
    "\n",
    "#         self.norm2 = nn.LayerNorm(emb_size)\n",
    "#         self.ffn = ResidualAdd(FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=drop_p))\n",
    "#         self.dropout2 = nn.Dropout(drop_p)\n",
    "# patches_embedded = PatchEmbedding()(x)\n",
    "    # def forward(self, x):\n",
    "    #     x = self.attn(self.norm1(x))  # MHA + æ®‹å·®\n",
    "    #     x = self.dropout1(x)\n",
    "    #     x = self.ffn(self.norm2(x))  # FFN + æ®‹å·®\n",
    "    #     x = self.dropout2(x)\n",
    "    #     return x\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "    ))\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "# print(TransformerEncoderBlock()(patches_embedded).shape) # torch.Size([1, 197, 768])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x)  # ä¾æ¬¡é€šè¿‡æ¯ä¸ª Transformer Encoder Block\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ†ç±»å¤´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ğŸ”„ åŠ è½½æœ€æ–°çš„ Checkpoint: training_dir/vit_epoch_1.pth\n",
      "âœ… Checkpoint training_dir/vit_epoch_1.pth åŠ è½½æˆåŠŸï¼Œä» Epoch 2 ç»§ç»­è®­ç»ƒï¼\n",
      "Epoch 2 å¼€å§‹è®­ç»ƒ...\n",
      "Train Epoch: 2 [0/50000] Loss: 1.744868\n",
      "Train Epoch: 2 [160/50000] Loss: 1.569748\n",
      "Train Epoch: 2 [320/50000] Loss: 1.814432\n",
      "Train Epoch: 2 [480/50000] Loss: 1.787008\n",
      "Train Epoch: 2 [640/50000] Loss: 2.029443\n",
      "Train Epoch: 2 [800/50000] Loss: 2.434287\n",
      "Train Epoch: 2 [960/50000] Loss: 1.763276\n",
      "Train Epoch: 2 [1120/50000] Loss: 1.773651\n",
      "Train Epoch: 2 [1280/50000] Loss: 2.174492\n",
      "Train Epoch: 2 [1440/50000] Loss: 2.105923\n",
      "Train Epoch: 2 [1600/50000] Loss: 2.190749\n",
      "Train Epoch: 2 [1760/50000] Loss: 2.200714\n",
      "Train Epoch: 2 [1920/50000] Loss: 1.745773\n",
      "Train Epoch: 2 [2080/50000] Loss: 2.010264\n",
      "Train Epoch: 2 [2240/50000] Loss: 1.897064\n",
      "Train Epoch: 2 [2400/50000] Loss: 2.396065\n",
      "Train Epoch: 2 [2560/50000] Loss: 2.297965\n",
      "Train Epoch: 2 [2720/50000] Loss: 2.259289\n",
      "Train Epoch: 2 [2880/50000] Loss: 1.870618\n",
      "Train Epoch: 2 [3040/50000] Loss: 1.954506\n",
      "Train Epoch: 2 [3200/50000] Loss: 1.856021\n",
      "Train Epoch: 2 [3360/50000] Loss: 2.284893\n",
      "Train Epoch: 2 [3520/50000] Loss: 2.232515\n",
      "Train Epoch: 2 [3680/50000] Loss: 1.982586\n",
      "Train Epoch: 2 [3840/50000] Loss: 1.954264\n",
      "Train Epoch: 2 [4000/50000] Loss: 1.757510\n",
      "Train Epoch: 2 [4160/50000] Loss: 2.107173\n",
      "Train Epoch: 2 [4320/50000] Loss: 1.767241\n",
      "Train Epoch: 2 [4480/50000] Loss: 2.168341\n",
      "Train Epoch: 2 [4640/50000] Loss: 2.236884\n",
      "Train Epoch: 2 [4800/50000] Loss: 1.952621\n",
      "Train Epoch: 2 [4960/50000] Loss: 1.751391\n",
      "Train Epoch: 2 [5120/50000] Loss: 2.307239\n",
      "Train Epoch: 2 [5280/50000] Loss: 2.235882\n",
      "Train Epoch: 2 [5440/50000] Loss: 1.893474\n",
      "Train Epoch: 2 [5600/50000] Loss: 1.913245\n",
      "Train Epoch: 2 [5760/50000] Loss: 1.952348\n",
      "Train Epoch: 2 [5920/50000] Loss: 2.210131\n",
      "Train Epoch: 2 [6080/50000] Loss: 2.116408\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# âœ… è®­ç»ƒç›®å½•\n",
    "checkpoint_dir = Path(\"training_dir\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# âœ… è®¾å¤‡\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# âœ… é¢„å¤„ç†ï¼ˆæ•°æ®å¢å¼ºï¼‰\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # é˜²æ­¢ Patch è¿‡åº¦è£å‰ª\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# âœ… åŠ è½½ CIFAR-10 æ•°æ®é›†\n",
    "train_dataset = datasets.CIFAR10(root=checkpoint_dir, train=True, download=False, transform=transform)\n",
    "# ä¿®æ”¹åï¼ˆæµ‹è¯•é›†ï¼‰ï¼Œå’Œè®­ç»ƒé›†ä¿æŒä¸€è‡´\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=checkpoint_dir, \n",
    "    train=False, \n",
    "    download=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # è¿™é‡Œå¯ä»¥åŠ ä¸Šä¸è®­ç»ƒé›†ä¸€è‡´çš„ Normalize\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# âœ… **å®ä¾‹åŒ–æ¨¡å‹**\n",
    "model = ViT(\n",
    "    in_channels=3,\n",
    "    patch_size=16,  # âœ… é€‚åº” CIFAR-10\n",
    "    emb_size=768,\n",
    "    img_size=224,\n",
    "    depth=12,\n",
    "    n_classes=10\n",
    ").to(device)\n",
    "\n",
    "# âœ… **ä¼˜åŒ–å™¨**\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# âœ… **å­¦ä¹ ç‡è°ƒåº¦å™¨**\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "# âœ… å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# âœ… **æ£€æŸ¥æ˜¯å¦æœ‰å·²è®­ç»ƒçš„ Checkpoint**\n",
    "checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")])\n",
    "latest_checkpoint = checkpoint_dir / checkpoint_files[-1] if checkpoint_files else None\n",
    "\n",
    "# âœ… **å¦‚æœå­˜åœ¨ Checkpointï¼Œåˆ™åŠ è½½**\n",
    "start_epoch = 1\n",
    "if latest_checkpoint:\n",
    "    print(f\"ğŸ”„ åŠ è½½æœ€æ–°çš„ Checkpoint: {latest_checkpoint}\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)  # âœ… å¿½ç•¥ shape ä¸åŒ¹é…\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹è®­ç»ƒ\n",
    "    print(f\"âœ… Checkpoint {latest_checkpoint} åŠ è½½æˆåŠŸï¼Œä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒï¼\")\n",
    "\n",
    "\n",
    "# âœ… è®­ç»ƒè¿‡ç¨‹\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch} å¼€å§‹è®­ç»ƒ...\")\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # âœ… æ¯ 10 ä¸ª Batch æ‰“å°ä¸€æ¬¡ Loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "    scheduler.step()  # âœ… æ›´æ–°å­¦ä¹ ç‡\n",
    "\n",
    "    # âœ… è®­ç»ƒå®Œæˆåï¼Œä¿å­˜ Checkpoint\n",
    "    checkpoint_path = checkpoint_dir / f\"vit_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"âœ… Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "# âœ… æµ‹è¯•è¿‡ç¨‹\n",
    "def test(model, device, test_loader, criterion):\n",
    "    print(\"testing\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # if data.shape[1:] != (3, 224, 224):  # âœ… æ£€æŸ¥æ•°æ® shape\n",
    "            #     raise ValueError(f\"Test batch shape mismatch: {data.shape}\")\n",
    "\n",
    "            output = model(data)  # âœ… ç¡®ä¿è¾“å…¥ä¸€è‡´\n",
    "\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n\")\n",
    "\n",
    "\n",
    "# âœ… è®­ç»ƒå¾ªç¯ï¼ˆæ‰©å±•åˆ° 50 Epochï¼‰\n",
    "num_epochs = 50\n",
    "for epoch in range(start_epoch, num_epochs + 1):  # âœ… ä» start_epoch å¼€å§‹\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
